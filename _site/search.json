[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R_locker",
    "section": "",
    "text": "통계학과를 다니면서 배운 내용을 정리한 곳입니다!\n여기는 R을 사용한 자료만 모여 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n(수업) 시계열 자료분석 실습 5\n\n\n\n\n\n\n\nR\n\n\nlesson\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\n조윤호\n\n\n\n\n\n\n\n\n(수업) 시계열 자료분석 실습 4\n\n\n\n\n\n\n\nR\n\n\nlesson\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n조윤호\n\n\n\n\n\n\n\n\n(수업) 시계열 자료분석 실습 3\n\n\n\n\n\n\n\nR\n\n\nlesson\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2022\n\n\n조윤호\n\n\n\n\n\n\n\n\n(수업) 시계열 자료분석 실습 2\n\n\n\n\n\n\n\nR\n\n\nlesson\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\n조윤호\n\n\n\n\n\n\n\n\n(수업) 시계열 자료분석 실습 1\n\n\n\n\n\n\n\nR\n\n\nlesson\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2022\n\n\n조윤호\n\n\n\n\n\n\n\n\n회귀분석\n\n\n\n\n\n\n\nR\n\n\ntheory\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\n조윤호\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "통계학과 3학년의 학습내용을 담은 블로그입니다."
  },
  {
    "objectID": "posts/2022-08-10_회귀분석.html",
    "href": "posts/2022-08-10_회귀분석.html",
    "title": "Yun98",
    "section": "",
    "text": "toc:true\nbranch: master\nbadges: true\ncomments: true\nauthor: 조윤호\n배운 내용정리\n\n\nlibrary('tidyverse')\n\n\n\n\n\n\nex1 <- tribble(\n    ~x, ~y,\n    1, 150,\n    2, 160,\n    3, 170,\n    4, 150,\n    5, 140,\n    6, 160,\n    7, 190\n    )\n\n\nex1\n\n\n\nA tibble: 7 × 2\n\n    xy\n    <dbl><dbl>\n\n\n    1150\n    2160\n    3170\n    4150\n    5140\n    6160\n    7190\n\n\n\n\n\nhistplot\n\n\nex1$y %>% hist()\n\n\n\n\n\n히스토그램은 계급 구간의 수가 중요하다(보통 10개 내외 사용)\n\n\nboxplot\n\n\nex1$y %>% boxplot()\n\n\n\n\n맨 위아래 가로바 밖은 이상치(outlier)들 박스 맨 아래는 Q1, 가운데 선은 Q2(평균), 박스 맨 위는 Q3를 의미한다. 이 때 Q3-Q1 = h라고 하며 맨 위의 가로바, 맨 아래의 가로바는 각각 Q3 + 1.5h, Q1 - 1.5h인 지점이다.\n\n\n\n모수(parameter) : 모집단이나 변수의 통계적 특성을 어떤 수치로 표현한 것 모수의 예로는 모집단의 중심위치를 나타내는 평균, 중간값, 최빈값과 값들이 중심에서 퍼져있는 정도, 즉 산포도를 나타내는 범위, 편차, 표준편차, 분산등이 있다. 이들 모수의 값은 대부분의 경우 알려져 있지 않으므로 표본을 이용하여 이들 값을 추정하게 된다.  통계랑(statisic) : 모수에 대응하여 표본의 특성을 잘 나타내는 수치 통계량은 표본을 이용하여 계산되므로 각 모수에다 ’표본’을 붙여 통계량을 나타낸다.ex) 표본 평균, 표본 분산 두 개 이상의 변수들의 관계에 대한 측도로는 공분산과 상관계수 등이 사용된다.\n\n\n\n확률변수(random variable) : 분석대상인 변수들이 갖는 각 결과에 하나의 실수값을 대응시켜주는 함수 확률(random)의 의미 : 확률변수의 값은 실제로 관측하기 전에는 어떠한 값이 될지 알 수 없다 확률분포(probability distribution) : 확률변수의 모든 가능한 값에 확률을 대응시킨 것 확률분포는 이산확률변수의 경우 표, 그림, 수식으로 나타내어지고, 연속확률변수의 경우 그림, 수식으로 주어진다.\n\n\n\\(E[Y] = \\sum\\limits_{i=1}^d{y_if_Y(y_i)}\\)\n기댓값은 다음과 같이 정의 되는데 위의 의미는 이산확률변수 Y의 기댓값은 \\(f_y(y_i)\\)가 가중치로 주어지는 가중평균이라는 것이다.\n\n\n\n공분산(covariance)는 두 변수간의 선형관계를 나타내는 측도로 다음과 같이 정의된다.\n\\(cov(Y_1, Y_2) = E[(Y_1 - \\mu Y_1)(Y_2 - \\mu Y_2)]\\)\n\n\n\n\\(\\rho = \\frac{cov(Y_1, Y_2)}{\\sigma Y1, \\sigma Y2}\\)\n상관계수 \\(\\rho\\)는 다음과 같이 해석된다. > 1) \\(\\rho\\)는 변수의 종류나 특정단위에 관계없는 측도로 -1과 +1 사이의 값을 가지며, \\(\\rho\\)의 값이 +1에 가까울수록 강한 양의 상관관계를,-1에 가까울수록 강한 음의 상관관계를 나타내며, \\(\\rho\\)의 값이 0에 가까울수록 선형관계는 약해진다. 2) \\(Y_1\\)과 \\(Y_2\\)의 대응되는 모든 값들이 한 직선 상에 위치하면 \\(\\rho\\)의 값은 -1이나 +1의 값을 가진다. 3) 상관계수 \\(\\rho\\)는 단지 두 변수간의 선형관계만을 나타내는 측도이다. 그러므로 \\(\\rho\\) = 0인 경우에 두 변수의 선형상관관계는 없지만 다른 관계는 가질 수 있다.\n\n\n\n여러가지 이산확률분포들 중에서 회귀분석과 관련이 있는 분포는 이항분포(binomial distribution)와 포아송분포(poisson distribution)이다. 먼저 이항분포는 베르누이 시행(bernoulli trial)에 의해 정의되는데, 어느 실험이 1) 오직 두 가지의 가능한 결과만을 가지고, 2) 매 번 시행에서 성공의 확률이 같아야 하며, 3) 각 시행은 서로 독립이라는 세 가지 조건을 만족하면 이를 베르누이 시행이라 한다.\n동전던지기(앞 or 뒤), 품질검사(양품 or 불량품)가 이에 해당한다. 보통 일반적으로 베르누이 시행의 결과를 성공과 실패로 나타낸다.\n이항분포 : 베르누이 시행을 독립적으로 n번 반복할 때 나타나는 성공의 개수가 갖는 확률분포\n포아송분포 : 단위 시간당 또는 단위 영역당 발생하는 사건의 횟수를 나타내기 위한 분포\n예를들어, 어느 시간대에 가게에 찾아오는 고객의 수, 어느 도시에서 하루동안 발생하는 교통사고의 수 등은 포아송분포를 따른다. 사건발생 평균횟수가 \\(\\lambda\\)인 포아송분포를 따르는 확률변수 Y의 확률분포는 다음과 같이 주어진다.\n\\(f(y) = \\frac{e^{-\\lambda}\\lambda^y}{y!}\\)\n포아송분포의 특성은 평균과 분산이 모두 \\(\\lambda\\)로 같다.\n\n\n\n- 정규분포의 대표적인 성질 하나의 정규분포를 따르는 확률변수의 선형함수 역시 정규분포를 따른다. 즉, 확률변수 Y가 평균이 \\(\\mu\\)이고 분산이 \\(\\sigma^2\\)인 정규분포를 따를 때 선형함수 a + bY는 평균이 a + \\(b\\mu\\)이고 분산이 \\(b^2\\sigma^2\\)인 정규분포를 따른다. 이것 때문에 표준화가 가능함.\n정규분포는 통계학에서 가장 많이 사용되는 확률분포이다. 그러나 모순적인 것은 정규분포를 정확하게 따르는 변수는 현실적으로 없다는 것이다. 왜냐하면 정규분포는 우선(-\\(\\infty\\), \\(\\infty\\))에서 정의되어야하고, 모든 구간에서의 확률이 0보다 커야 한다는 조건이 주어지기 때문이다. 그러므로 이를테면 항상 양수값만을 갖는 학생들의 신장이 정규분포를 따른다고 가정하는 것은 엄격하게 보면 잘못된 것일 수 있다. 그렇지만 이 가정이 타당성을 갖는 이유는 정규분포에서는 (\\(\\mu -3\\sigma, \\mu +3\\sigma\\))정도의 구간에 99.74%라는 거의 모든 확률이 포함되어 있기 때문이다. 신장의 경우 표준편차 \\(\\sigma\\)가 아주 크지 않는 경우에는 음수부분은 거의 확률이 존재하지 않는 것으로 보아도 무방하게 된다.\n중심극한정리(central limit theorem) : 확률변수 \\(Y1, Y2, ..., Y_n\\)들이 평균이 \\(\\mu\\)이고 분산이 \\(\\sigma^2 < \\infty\\)인 확률분포를 따르며  서로 독립일 때, 표본의 크기 n이 적당히 크면 표본평균의 분포는 근사적으로 정규분포에 가까워진다."
  },
  {
    "objectID": "posts/2022-08-10/2022-08-10_회귀분석.html",
    "href": "posts/2022-08-10/2022-08-10_회귀분석.html",
    "title": "Yun98",
    "section": "",
    "text": "toc:true\nbranch: master\nbadges: true\ncomments: true\nauthor: 조윤호\n배운 내용정리\n\n\nlibrary('tidyverse')\n\n\n\n\n\n\nex1 <- tribble(\n    ~x, ~y,\n    1, 150,\n    2, 160,\n    3, 170,\n    4, 150,\n    5, 140,\n    6, 160,\n    7, 190\n    )\n\n\nex1\n\n\n\nA tibble: 7 × 2\n\n    xy\n    <dbl><dbl>\n\n\n    1150\n    2160\n    3170\n    4150\n    5140\n    6160\n    7190\n\n\n\n\n\nhistplot\n\n\nex1$y %>% hist()\n\n\n\n\n\n히스토그램은 계급 구간의 수가 중요하다(보통 10개 내외 사용)\n\n\nboxplot\n\n\nex1$y %>% boxplot()\n\n\n\n\n맨 위아래 가로바 밖은 이상치(outlier)들 박스 맨 아래는 Q1, 가운데 선은 Q2(평균), 박스 맨 위는 Q3를 의미한다. 이 때 Q3-Q1 = h라고 하며 맨 위의 가로바, 맨 아래의 가로바는 각각 Q3 + 1.5h, Q1 - 1.5h인 지점이다.\n\n\n\n모수(parameter) : 모집단이나 변수의 통계적 특성을 어떤 수치로 표현한 것 모수의 예로는 모집단의 중심위치를 나타내는 평균, 중간값, 최빈값과 값들이 중심에서 퍼져있는 정도, 즉 산포도를 나타내는 범위, 편차, 표준편차, 분산등이 있다. 이들 모수의 값은 대부분의 경우 알려져 있지 않으므로 표본을 이용하여 이들 값을 추정하게 된다.  통계랑(statisic) : 모수에 대응하여 표본의 특성을 잘 나타내는 수치 통계량은 표본을 이용하여 계산되므로 각 모수에다 ’표본’을 붙여 통계량을 나타낸다.ex) 표본 평균, 표본 분산 두 개 이상의 변수들의 관계에 대한 측도로는 공분산과 상관계수 등이 사용된다.\n\n\n\n확률변수(random variable) : 분석대상인 변수들이 갖는 각 결과에 하나의 실수값을 대응시켜주는 함수 확률(random)의 의미 : 확률변수의 값은 실제로 관측하기 전에는 어떠한 값이 될지 알 수 없다 확률분포(probability distribution) : 확률변수의 모든 가능한 값에 확률을 대응시킨 것 확률분포는 이산확률변수의 경우 표, 그림, 수식으로 나타내어지고, 연속확률변수의 경우 그림, 수식으로 주어진다.\n\n\n\\(E[Y] = \\sum\\limits_{i=1}^d{y_if_Y(y_i)}\\)\n기댓값은 다음과 같이 정의 되는데 위의 의미는 이산확률변수 Y의 기댓값은 \\(f_y(y_i)\\)가 가중치로 주어지는 가중평균이라는 것이다.\n\n\n\n공분산(covariance)는 두 변수간의 선형관계를 나타내는 측도로 다음과 같이 정의된다.\n\\(cov(Y_1, Y_2) = E[(Y_1 - \\mu Y_1)(Y_2 - \\mu Y_2)]\\)\n\n\n\n\\(\\rho = \\frac{cov(Y_1, Y_2)}{\\sigma Y1, \\sigma Y2}\\)\n상관계수 \\(\\rho\\)는 다음과 같이 해석된다. > 1) \\(\\rho\\)는 변수의 종류나 특정단위에 관계없는 측도로 -1과 +1 사이의 값을 가지며, \\(\\rho\\)의 값이 +1에 가까울수록 강한 양의 상관관계를,-1에 가까울수록 강한 음의 상관관계를 나타내며, \\(\\rho\\)의 값이 0에 가까울수록 선형관계는 약해진다. 2) \\(Y_1\\)과 \\(Y_2\\)의 대응되는 모든 값들이 한 직선 상에 위치하면 \\(\\rho\\)의 값은 -1이나 +1의 값을 가진다. 3) 상관계수 \\(\\rho\\)는 단지 두 변수간의 선형관계만을 나타내는 측도이다. 그러므로 \\(\\rho\\) = 0인 경우에 두 변수의 선형상관관계는 없지만 다른 관계는 가질 수 있다.\n\n\n\n여러가지 이산확률분포들 중에서 회귀분석과 관련이 있는 분포는 이항분포(binomial distribution)와 포아송분포(poisson distribution)이다. 먼저 이항분포는 베르누이 시행(bernoulli trial)에 의해 정의되는데, 어느 실험이 1) 오직 두 가지의 가능한 결과만을 가지고, 2) 매 번 시행에서 성공의 확률이 같아야 하며, 3) 각 시행은 서로 독립이라는 세 가지 조건을 만족하면 이를 베르누이 시행이라 한다.\n동전던지기(앞 or 뒤), 품질검사(양품 or 불량품)가 이에 해당한다. 보통 일반적으로 베르누이 시행의 결과를 성공과 실패로 나타낸다.\n이항분포 : 베르누이 시행을 독립적으로 n번 반복할 때 나타나는 성공의 개수가 갖는 확률분포\n포아송분포 : 단위 시간당 또는 단위 영역당 발생하는 사건의 횟수를 나타내기 위한 분포\n예를들어, 어느 시간대에 가게에 찾아오는 고객의 수, 어느 도시에서 하루동안 발생하는 교통사고의 수 등은 포아송분포를 따른다. 사건발생 평균횟수가 \\(\\lambda\\)인 포아송분포를 따르는 확률변수 Y의 확률분포는 다음과 같이 주어진다.\n\\(f(y) = \\frac{e^{-\\lambda}\\lambda^y}{y!}\\)\n포아송분포의 특성은 평균과 분산이 모두 \\(\\lambda\\)로 같다.\n\n\n\n- 정규분포의 대표적인 성질 하나의 정규분포를 따르는 확률변수의 선형함수 역시 정규분포를 따른다. 즉, 확률변수 Y가 평균이 \\(\\mu\\)이고 분산이 \\(\\sigma^2\\)인 정규분포를 따를 때 선형함수 a + bY는 평균이 a + \\(b\\mu\\)이고 분산이 \\(b^2\\sigma^2\\)인 정규분포를 따른다. 이것 때문에 표준화가 가능함.\n정규분포는 통계학에서 가장 많이 사용되는 확률분포이다. 그러나 모순적인 것은 정규분포를 정확하게 따르는 변수는 현실적으로 없다는 것이다. 왜냐하면 정규분포는 우선(-\\(\\infty\\), \\(\\infty\\))에서 정의되어야하고, 모든 구간에서의 확률이 0보다 커야 한다는 조건이 주어지기 때문이다. 그러므로 이를테면 항상 양수값만을 갖는 학생들의 신장이 정규분포를 따른다고 가정하는 것은 엄격하게 보면 잘못된 것일 수 있다. 그렇지만 이 가정이 타당성을 갖는 이유는 정규분포에서는 (\\(\\mu -3\\sigma, \\mu +3\\sigma\\))정도의 구간에 99.74%라는 거의 모든 확률이 포함되어 있기 때문이다. 신장의 경우 표준편차 \\(\\sigma\\)가 아주 크지 않는 경우에는 음수부분은 거의 확률이 존재하지 않는 것으로 보아도 무방하게 된다.\n중심극한정리(central limit theorem) : 확률변수 \\(Y1, Y2, ..., Y_n\\)들이 평균이 \\(\\mu\\)이고 분산이 \\(\\sigma^2 < \\infty\\)인 확률분포를 따르며  서로 독립일 때, 표본의 크기 n이 적당히 크면 표본평균의 분포는 근사적으로 정규분포에 가까워진다."
  },
  {
    "objectID": "posts/2022-08-10/2022-08-10_회귀분석.html",
    "href": "posts/2022-08-10/2022-08-10_회귀분석.html",
    "title": "Yun98",
    "section": "",
    "text": "title: “회귀”\nauthor: “YunHo”\ndate: “2022-11-01”\ncategories: [news, code, analysis]\nimage: “image.jpg”\n배운 내용정리\n\n\nlibrary('tidyverse')\n\n\n\n\n\n\nex1 <- tribble(\n    ~x, ~y,\n    1, 150,\n    2, 160,\n    3, 170,\n    4, 150,\n    5, 140,\n    6, 160,\n    7, 190\n    )\n\n\nex1\n\n\n\nA tibble: 7 × 2\n\n    xy\n    <dbl><dbl>\n\n\n    1150\n    2160\n    3170\n    4150\n    5140\n    6160\n    7190\n\n\n\n\n\nhistplot\n\n\nex1$y %>% hist()\n\n\n\n\n\n히스토그램은 계급 구간의 수가 중요하다(보통 10개 내외 사용)\n\n\nboxplot\n\n\nex1$y %>% boxplot()\n\n\n\n\n맨 위아래 가로바 밖은 이상치(outlier)들 박스 맨 아래는 Q1, 가운데 선은 Q2(평균), 박스 맨 위는 Q3를 의미한다. 이 때 Q3-Q1 = h라고 하며 맨 위의 가로바, 맨 아래의 가로바는 각각 Q3 + 1.5h, Q1 - 1.5h인 지점이다.\n\n\n\n모수(parameter) : 모집단이나 변수의 통계적 특성을 어떤 수치로 표현한 것 모수의 예로는 모집단의 중심위치를 나타내는 평균, 중간값, 최빈값과 값들이 중심에서 퍼져있는 정도, 즉 산포도를 나타내는 범위, 편차, 표준편차, 분산등이 있다. 이들 모수의 값은 대부분의 경우 알려져 있지 않으므로 표본을 이용하여 이들 값을 추정하게 된다.  통계랑(statisic) : 모수에 대응하여 표본의 특성을 잘 나타내는 수치 통계량은 표본을 이용하여 계산되므로 각 모수에다 ’표본’을 붙여 통계량을 나타낸다.ex) 표본 평균, 표본 분산 두 개 이상의 변수들의 관계에 대한 측도로는 공분산과 상관계수 등이 사용된다.\n\n\n\n확률변수(random variable) : 분석대상인 변수들이 갖는 각 결과에 하나의 실수값을 대응시켜주는 함수 확률(random)의 의미 : 확률변수의 값은 실제로 관측하기 전에는 어떠한 값이 될지 알 수 없다 확률분포(probability distribution) : 확률변수의 모든 가능한 값에 확률을 대응시킨 것 확률분포는 이산확률변수의 경우 표, 그림, 수식으로 나타내어지고, 연속확률변수의 경우 그림, 수식으로 주어진다.\n\n\n\\(E[Y] = \\sum\\limits_{i=1}^d{y_if_Y(y_i)}\\)\n기댓값은 다음과 같이 정의 되는데 위의 의미는 이산확률변수 Y의 기댓값은 \\(f_y(y_i)\\)가 가중치로 주어지는 가중평균이라는 것이다.\n\n\n\n공분산(covariance)는 두 변수간의 선형관계를 나타내는 측도로 다음과 같이 정의된다.\n\\(cov(Y_1, Y_2) = E[(Y_1 - \\mu Y_1)(Y_2 - \\mu Y_2)]\\)\n\n\n\n\\(\\rho = \\frac{cov(Y_1, Y_2)}{\\sigma Y1, \\sigma Y2}\\)\n상관계수 \\(\\rho\\)는 다음과 같이 해석된다. > 1) \\(\\rho\\)는 변수의 종류나 특정단위에 관계없는 측도로 -1과 +1 사이의 값을 가지며, \\(\\rho\\)의 값이 +1에 가까울수록 강한 양의 상관관계를,-1에 가까울수록 강한 음의 상관관계를 나타내며, \\(\\rho\\)의 값이 0에 가까울수록 선형관계는 약해진다. 2) \\(Y_1\\)과 \\(Y_2\\)의 대응되는 모든 값들이 한 직선 상에 위치하면 \\(\\rho\\)의 값은 -1이나 +1의 값을 가진다. 3) 상관계수 \\(\\rho\\)는 단지 두 변수간의 선형관계만을 나타내는 측도이다. 그러므로 \\(\\rho\\) = 0인 경우에 두 변수의 선형상관관계는 없지만 다른 관계는 가질 수 있다.\n\n\n\n여러가지 이산확률분포들 중에서 회귀분석과 관련이 있는 분포는 이항분포(binomial distribution)와 포아송분포(poisson distribution)이다. 먼저 이항분포는 베르누이 시행(bernoulli trial)에 의해 정의되는데, 어느 실험이 1) 오직 두 가지의 가능한 결과만을 가지고, 2) 매 번 시행에서 성공의 확률이 같아야 하며, 3) 각 시행은 서로 독립이라는 세 가지 조건을 만족하면 이를 베르누이 시행이라 한다.\n동전던지기(앞 or 뒤), 품질검사(양품 or 불량품)가 이에 해당한다. 보통 일반적으로 베르누이 시행의 결과를 성공과 실패로 나타낸다.\n이항분포 : 베르누이 시행을 독립적으로 n번 반복할 때 나타나는 성공의 개수가 갖는 확률분포\n포아송분포 : 단위 시간당 또는 단위 영역당 발생하는 사건의 횟수를 나타내기 위한 분포\n예를들어, 어느 시간대에 가게에 찾아오는 고객의 수, 어느 도시에서 하루동안 발생하는 교통사고의 수 등은 포아송분포를 따른다. 사건발생 평균횟수가 \\(\\lambda\\)인 포아송분포를 따르는 확률변수 Y의 확률분포는 다음과 같이 주어진다.\n\\(f(y) = \\frac{e^{-\\lambda}\\lambda^y}{y!}\\)\n포아송분포의 특성은 평균과 분산이 모두 \\(\\lambda\\)로 같다.\n\n\n\n- 정규분포의 대표적인 성질 하나의 정규분포를 따르는 확률변수의 선형함수 역시 정규분포를 따른다. 즉, 확률변수 Y가 평균이 \\(\\mu\\)이고 분산이 \\(\\sigma^2\\)인 정규분포를 따를 때 선형함수 a + bY는 평균이 a + \\(b\\mu\\)이고 분산이 \\(b^2\\sigma^2\\)인 정규분포를 따른다. 이것 때문에 표준화가 가능함.\n정규분포는 통계학에서 가장 많이 사용되는 확률분포이다. 그러나 모순적인 것은 정규분포를 정확하게 따르는 변수는 현실적으로 없다는 것이다. 왜냐하면 정규분포는 우선(-\\(\\infty\\), \\(\\infty\\))에서 정의되어야하고, 모든 구간에서의 확률이 0보다 커야 한다는 조건이 주어지기 때문이다. 그러므로 이를테면 항상 양수값만을 갖는 학생들의 신장이 정규분포를 따른다고 가정하는 것은 엄격하게 보면 잘못된 것일 수 있다. 그렇지만 이 가정이 타당성을 갖는 이유는 정규분포에서는 (\\(\\mu -3\\sigma, \\mu +3\\sigma\\))정도의 구간에 99.74%라는 거의 모든 확률이 포함되어 있기 때문이다. 신장의 경우 표준편차 \\(\\sigma\\)가 아주 크지 않는 경우에는 음수부분은 거의 확률이 존재하지 않는 것으로 보아도 무방하게 된다.\n중심극한정리(central limit theorem) : 확률변수 \\(Y1, Y2, ..., Y_n\\)들이 평균이 \\(\\mu\\)이고 분산이 \\(\\sigma^2 < \\infty\\)인 확률분포를 따르며  서로 독립일 때, 표본의 크기 n이 적당히 크면 표본평균의 분포는 근사적으로 정규분포에 가까워진다."
  },
  {
    "objectID": "posts/2022-10-02-시계열자료분석-학습1.html",
    "href": "posts/2022-10-02-시계열자료분석-학습1.html",
    "title": "Yun98",
    "section": "",
    "text": "불규칙, 추세성분, 주기성분의 이해 - toc:true - branch: master - badges: true - comments: true - author: 조윤호\n\n\nlibrary('data.table')\nlibrary('tidyverse')\nlibrary('gridExtra') # grid.arrange 사용해주는(그림 한 화면에 그려주는, 사실 jupyter에서는 별로 쓸모 없음) \nlibrary('lmtest') # dwtest\n\n\n\n\nset.seed(1245)\nn = 100\n\n\nset.seed(1245)\nz <- 5000 + 20*rnorm(n)\nz %>% head\n\n\n5008.382801322284970.883327897544970.882398360754979.687072561944989.338017215614986.32787849186\n\n\n- 같은 방법\n\nset.seed(1245)\nrnorm(n, 5000, 20) %>% head\n\n\n5008.382801322284970.883327897544970.882398360754979.687072561944989.338017215614986.32787849186\n\n\n\nplot(rnorm(n), type = 'l')\nabline(h = 0, lty = 2)\n\n\n\n\n\n20곱하면 표준편차가 +- 20정도 증가\n\n\nplot(20*rnorm(n), type = 'l')\nabline(h = 0, lty = 2)\n\n\n\n\n\n5000더하면 평균도 5000증가\n\n\nplot(5000 + 20*rnorm(n), type = 'l')\nabline(h = 5000, lty = 2)\n\n\n\n\nts : TimeSeries로 바꿔주는 함수(시계열)\n\nz.ts <- ts(z,\n           start = c(1980,1), # 1980년 1월부터 시작\n           frequency = 12) # 12면 월별로, 4면 분기별로, 365면 일별로\nz.ts\n\n\n\nA Time Series: 9 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    19805008.3834970.8834970.8824979.6874989.3384986.3285006.5185018.4514998.9835005.9444988.9784985.049\n    19815017.0354999.1034970.2894994.1244962.2044986.3115013.9224970.4944988.5714978.3634974.5295009.565\n    19824976.2174996.0124966.7555029.4895018.5755011.3985013.2095023.4214997.9015012.8204982.2455003.137\n    19835002.3525027.8035009.0284978.8744992.8705006.6864995.8024985.5614984.2865000.3535002.7724989.359\n    19845030.0114985.2724999.7675020.0354974.9765043.3494966.2285003.0274976.1265030.5124983.9015016.143\n    19854966.1775044.2414999.5144995.4454992.8184985.3865009.7784980.8184979.3394983.1665010.1625000.873\n    19864979.7124973.1134994.0885004.4155024.6815001.0015019.1754964.4174994.1124971.1705021.8705015.610\n    19874996.4874984.2194967.2795008.6345032.1914992.3095005.2094999.6485025.7374996.7664985.4084994.681\n    19885013.3425027.1085012.3205010.116                                                                \n\n\n\n\n\nz.ts %>% cycle # 주기알려줌\n\n\n\nA Time Series: 9 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    1980 1 2 3 4 5 6 7 8 9101112\n    1981 1 2 3 4 5 6 7 8 9101112\n    1982 1 2 3 4 5 6 7 8 9101112\n    1983 1 2 3 4 5 6 7 8 9101112\n    1984 1 2 3 4 5 6 7 8 9101112\n    1985 1 2 3 4 5 6 7 8 9101112\n    1986 1 2 3 4 5 6 7 8 9101112\n    1987 1 2 3 4 5 6 7 8 9101112\n    1988 1 2 3 4                \n\n\n\n\n\nz.ts %>% time\n\n\n\nA Time Series: 9 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    19801980.0001980.0831980.1671980.2501980.3331980.4171980.5001980.5831980.6671980.7501980.8331980.917\n    19811981.0001981.0831981.1671981.2501981.3331981.4171981.5001981.5831981.6671981.7501981.8331981.917\n    19821982.0001982.0831982.1671982.2501982.3331982.4171982.5001982.5831982.6671982.7501982.8331982.917\n    19831983.0001983.0831983.1671983.2501983.3331983.4171983.5001983.5831983.6671983.7501983.8331983.917\n    19841984.0001984.0831984.1671984.2501984.3331984.4171984.5001984.5831984.6671984.7501984.8331984.917\n    19851985.0001985.0831985.1671985.2501985.3331985.4171985.5001985.5831985.6671985.7501985.8331985.917\n    19861986.0001986.0831986.1671986.2501986.3331986.4171986.5001986.5831986.6671986.7501986.8331986.917\n    19871987.0001987.0831987.1671987.2501987.3331987.4171987.5001987.5831987.6671987.7501987.8331987.917\n    19881988.0001988.0831988.1671988.250                                                                \n\n\n\n\n\nz.ts %>% frequency\n\n12\n\n\n\nz.ts %>% start\n\n\n19801\n\n\n\nz.ts %>% end\n\n\n19884\n\n\n\nz.ts %>% tsp # 시작, 끝, 주기 알려줌\n\n\n19801988.2512\n\n\n\nts장점 : 기본적으로 linetype = l로 해줌, 부가 옵션들이 더 나옴\n\n\nts.plot(z.ts, xlab = \"date\", ylab = \"zt\",\n       main =\"irregular elements\")\nabline(h = 5000)\n\n\n\n\n- 예제)\n\na_ <- \"1980/1/1\"\na_ %>% class\nas.Date(a_) %>% class\n\n'character'\n\n\n'Date'\n\n\n\n보이기에 비슷해 보이는데 명확하게 분류되는게 작동면에서 유리\n\n\ntmp.data <- data.table(Time = seq.Date(as.Date(\"1980/1/1\"),\n                                       by = \"month\",\n                                       length.out = 100),\n                       z = z)\ntmp.data %>% head\n\n\n\nA data.table: 6 × 2\n\n    Timez\n    <date><dbl>\n\n\n    1980-01-015008.383\n    1980-02-014970.883\n    1980-03-014970.882\n    1980-04-014979.687\n    1980-05-014989.338\n    1980-06-014986.328\n\n\n\n\n\nggplot(tmp.data, aes(Time, z)) +\ngeom_line(col = 'steelblue') +\ngeom_hline(yintercept = 5000, col = 'grey80', lty = 2) + # 선 긋기\nggtitle(\"irregular elements\") +\nscale_x_date(date_breaks = \"year\", date_labels = \"%Y\") + # 연 단위로 보이게 만들기\ntheme_bw() +\ntheme(text = element_text(size = 16), # 글씨 크기 조절 옵션\n      axis.title = element_blank()) # x, y label 제거\n\n\n\n\n\n\n\n\n하는 방법 : 일단 추세를 하나 그리고 노이즈를 더해준다.\n\n\nset.seed(1234)\nn = 100\nt <- 1:n\nx <- 0.5*t # 추세\nz <- 0.5*t + rnorm(n) # 추세 + 오차\n\n\nplot(x, type = 'l')\n\n\n\n\n- 기본 추세 + 노이즈\n\nplot(z, type = 'l')\n\n\n\n\n- 노이즈 추가, 연도, 주기, 변경\n\nz.ts <- ts(z, start = c(1980, 1), frequency = 12)\nx.ts <- ts(x, start = c(1980, 1), frequency = 12)\n\n- 추세선 + (추세 + 노이즈)선\n\nts.plot(z.ts, x.ts)\n\n\n\n\n- 추세선, 추세 + 오차선 색 다르게해서 그리기\n\nts.plot(z.ts, x.ts,\n        col = c('blue', 'red'),\n        lty = 1:2,\n        xlab = \"date\",\n        ylab = \"zt\",\n        main = \"trend component\")\n\nlegend(\"topleft\", # 범례 추가\n       legend = c(\"series\", \"trend\"),\n       lty = 1:2,\n       col = c(\"blue\", \"red\"))\n\n\n\n\n\n\n\n\nn = 120 # 계절성분 120개\nt <- 1:n\na <- rnorm(n,0,1) # 오차\n\n\nplot(a, type = 'l')\n\n\n\n\n\n앞에 0.8곱해주면 분산을 줄여줌(오차를 줄이니까) 10더한 것은 평균 10증가 -> 불규칙성분만들기\n\n\nplot(10 + 0.8*a, type = 'l')\n\n\n\n\n- 주기가 있는 사인함수\n\nplot(sin((2*pi*t)/12),type = 'l')\n\n\n\n\n- z는 위의 sin함수에 노이즈가 더해진 형태\n\nz <- 10 + 3*sin((2*pi*t)/12) + 0.8*a\n\n\nsin함수에 오차가 섞여서 계절성을 띄는 성분이 됨.\n\n\nz.ts <- ts(z,\n           start = c(1985, 1),\n           frequency = 12)\nplot(z.ts,\n     xlab = \"date\",\n     ylab = \"zt\",\n     main = \"seasonal component\")\n\n\n\n\n\n\n\n\n\n\nx <- seq(0, 48, 0.01)\ns <- 12\npar(mfrow = c(4, 1))\n\n\nplot(x, sin(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2)\n\nplot(x, cos(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('cos::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2)\n\n\n\n\n\n\n\n\nsin, cos 두 함수 더한 함수 아래는 각각 weight를 다르게 해 반영한 함수\n\n\nplot(x, sin(2*pi*x/s) + cos(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin+cos::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2)\nabline(v = seq(1.5, 48, by = s), lty = 2)\n\nplot(x, 1.5*sin(2*pi*x/s) + 0.7*cos(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin+cos::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = s), lty = 2)\nabline(v = seq(1.5, 24, by = s), lty = 2)\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(4, 1))\n\n\ns <- 12\nplot(x, sin(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2)\n\ns <- 6\nplot(x, sin(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2)\n\n\n\n\n\n\n\n\n주기는 6그대로인데 weight를 다르게하면 또 다른 다양한 형태가 나온다.\n\n\nplot(x, sin(2*pi*x/12) + sin(2*pi*x/6), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2) \n\nplot(x, 2*sin(2*pi*x/12) + 0.8*sin(2*pi*x/6), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2) \n\n\n\n\n\n\n\n\n\n\ns <- 12\nplot(x, sin(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = s), lty = 2)\n\ns <- 6\nplot(x, sin(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = s), lty = 2)\n\ns <- 3\nplot(x, sin(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = s), lty = 2)\n\nplot(x, sin(2*pi*x/12) + sin(2*pi*x/6) + sin(2*pi*x/3), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin + sin + sin::', \"frequncy=\", 12))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = s), lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n여러개 섞으면서 그리기 마지막 거는 추세까지 섞임\n\n\nplot(x, sin(2*pi*x/12) + sin(2*pi*x/6) + sin(2*pi*x/3), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin + sin + sin::', \"frequncy=\", 12))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = 12), lty = 2)\n\ny <- sin(2*pi*x/12) + sin(2*pi*x/6) + sin(2*pi*x/3) + cos(2*pi*x/12) + cos(2*pi*x/6) + cos(2*pi*x/3)\n\nplot(x, y, type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = \"s+s+s+c+c+c : frequncy=, 12\")\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = 12), lty = 2)\n\ny2 <- x*0.5 + sin(2*pi*x/12) + sin(2*pi*x/6) +sin(2*pi*x/3) + cos(2*pi*x/12) + cos(2*pi*x/6) + cos(2*pi*x/3)\n\nplot(x, y2, type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = \"s+s+s+c+c+c frequency=12\")\nabline(a = 0, b = 0.5, lty = 2)\nabline(v = seq(0, 24, by = 12), lty = 2)\n\n\n\n\n\n\n\n\n\n\n\nn <- 100\nt <- 1:n\na1 <- -0.8 # 진폭\na2 <- 1.4 # 진폭\nphi1 <- pi/3\nphi2 <- 3*pi/4\nfirst <- a1*sin(pi*t/6 + phi1) # 첫 번째 주기성분(주기 6)\nsecond <- a2*sin(pi*t/3 + phi2) # 두 번째 주기성분(주기 3)\n\n\ndt <- data.table(t = t,\n                 first = first, # 첫 번째 주기 성분\n                 second = second,# 두 번째 주기 성분\n                 z = first + second)# 그 두 개 더한 성분\n\np1 <- ggplot(dt, aes(t, first)) + geom_line(col = 'skyblue', size = 1) +\ngeom_point(col = 'steelblue', size = 1) +\nylim(-2.5, 2) + xlab(\"\") +\nscale_x_continuous(breaks = seq(1, 100, by = 12)) +\ntheme_bw()\n\np2 <- ggplot(dt, aes(t, second)) + geom_line(col = 'skyblue', size = 1) +\ngeom_point(col = 'steelblue', size = 1) +\nylim(-2.5, 2) + xlab(\"\") +\nscale_x_continuous(breaks = seq(1, 100, by = 12)) +\ntheme_bw()\n\np3 <- ggplot(dt, aes(t, z)) + geom_line(col = 'skyblue', size = 1) +\ngeom_point(col = 'steelblue', size = 1) +\nylim(-2.5, 2) + xlab(\"\") +\nscale_x_continuous(breaks = seq(1, 100, by = 12)) +\ntheme_bw()\n\n\np1\np2\np3\n\n\n\n\n\n\n\n\n\n\n\ngrid.arrange(p1, p2, p3, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nz <- scan(\"depart.txt\")\n\ndep <- ts(z, frequency = 12, start = c(1984, 1)) # TimeSeries로 바꿔도되고 안해도 되는데 여기서는 함\nplot(dep)\n\n\n\n\n\n매출액이 증가하는 추세가 보임 계졀성도 있어보임(특히 1년주기) 또 점점 분산이 증가하는 이분산성을 보이는데 따라서 로그 변환을 해준다.\n\n\ntmp.data <- data.table(\n    day = seq.Date(as.Date(\"1984-01-01\"),\n                   by = 'month', length.out = length(z)),\n    z = z\n    )\n\ntmp.data[, lndep := log(z)] # 로그변환\ntmp.data[, y := as.factor(as.integer(cycle(dep)))] # factor씌우는 게 그냥 넣으면 그대로 안 받아들여져서\ntmp.data[, trend := 1:length(z)]\n\ntmp.data %>% head\n\n\n\nA data.table: 6 × 5\n\n    dayzlndepytrend\n    <date><dbl><dbl><fct><int>\n\n\n    1984-01-014236.04737211\n    1984-02-014586.12686922\n    1984-03-016076.40852933\n    1984-04-015646.33505444\n    1984-05-015366.28413455\n    1984-06-015366.28413466\n\n\n\n\n\np1 <- ggplot(tmp.data, aes(day, z)) + \ngeom_line(col = 'skyblue') +\ngeom_point(col = 'steelblue') +\nscale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\nlabs(title = \"monthly department store sales TimeSeries plot\") +\ntheme_bw() +\ntheme(axis.title = element_blank())\n\np2 <- ggplot(tmp.data, aes(day, lndep)) + \ngeom_line(col = 'skyblue') +\ngeom_point(col = 'steelblue') +\nscale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\nlabs(title = \"monthly department store sales TimeSeries plot after log transformation\") +\ntheme_bw() +\ntheme(axis.title = element_blank())\n\n\n로그 변환 이후 분산 폭이 줄어듦을 볼 수 있다.\n\n\np1\np2 # 로그변환한 거\n\n\n\n\n\n\n\n\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n지시함수(Indicater)를 사용하고 싶다면, \\(\\beta_0\\) = 0 or \\(\\sum \\beta_i\\) = 0 or \\(\\beta_1\\) = 0 셋 중 하나를 가정해야함. 아래는 \\(\\beta_0\\) = 0 가정\n\n\ntmp.data %>% head\n\n\n\nA data.table: 6 × 5\n\n    dayzlndepytrend\n    <date><dbl><dbl><fct><int>\n\n\n    1984-01-014236.04737211\n    1984-02-014586.12686922\n    1984-03-016076.40852933\n    1984-04-015646.33505444\n    1984-05-015366.28413455\n    1984-06-015366.28413466\n\n\n\n\n\nreg <- lm(lndep ~ 0 + trend + y, data = tmp.data)\nsummary(reg)\n\n\nCall:\nlm(formula = lndep ~ 0 + trend + y, data = tmp.data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.038679 -0.018689 -0.001468  0.015185  0.057288 \n\nCoefficients:\n       Estimate Std. Error t value Pr(>|t|)    \ntrend 0.0106603  0.0001925   55.39   <2e-16 ***\ny1    6.0641904  0.0122952  493.21   <2e-16 ***\ny2    6.0807995  0.0123718  491.50   <2e-16 ***\ny3    6.3811183  0.0124509  512.50   <2e-16 ***\ny4    6.2953455  0.0125325  502.32   <2e-16 ***\ny5    6.2132392  0.0126164  492.47   <2e-16 ***\ny6    6.2197771  0.0127027  489.64   <2e-16 ***\ny7    6.5885065  0.0127914  515.08   <2e-16 ***\ny8    6.1842831  0.0128823  480.06   <2e-16 ***\ny9    6.1001148  0.0129754  470.13   <2e-16 ***\ny10   6.3334505  0.0130707  484.56   <2e-16 ***\ny11   6.3417116  0.0131681  481.60   <2e-16 ***\ny12   7.1104816  0.0132676  535.93   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.0253 on 47 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 3.199e+05 on 13 and 47 DF,  p-value: < 2.2e-16\n\n\n\n해석 : coefficients에서 trend 보면 매월 0.0106603씩 증가함을 볼 수 있음. 로그 취한 것이기에 6.0641904, 6.0807995등은 각각 로그매출액의 1월의 평균, 2월의 평균을 의미(쭉쭉 12월까지) 미국은 블랙 프라이데이등으로 인해 12월 매출이 높은 것을 볼 수 있음, 또 높은 구간은 여름(7월)이고 이는 그래프에서도 나타남.\n\n\n\\(\\beta_1\\) = 0 가정 위 처럼 0 안넣으면 자동으로 “\\(\\beta_1\\) = 0 가정” 들어간다.\n\n\nreg2 <- lm(lndep ~ trend + y, data = tmp.data)\nsummary(reg2)\ncontrasts(tmp.data$y)\n\n\nCall:\nlm(formula = lndep ~ trend + y, data = tmp.data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.038679 -0.018689 -0.001468  0.015185  0.057288 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 6.0641904  0.0122952 493.215  < 2e-16 ***\ntrend       0.0106603  0.0001925  55.388  < 2e-16 ***\ny2          0.0166091  0.0160024   1.038   0.3046    \ny3          0.3169279  0.0160059  19.801  < 2e-16 ***\ny4          0.2311551  0.0160117  14.437  < 2e-16 ***\ny5          0.1490488  0.0160198   9.304 3.12e-12 ***\ny6          0.1555867  0.0160302   9.706 8.32e-13 ***\ny7          0.5243161  0.0160429  32.682  < 2e-16 ***\ny8          0.1200927  0.0160579   7.479 1.54e-09 ***\ny9          0.0359244  0.0160752   2.235   0.0302 *  \ny10         0.2692601  0.0160948  16.730  < 2e-16 ***\ny11         0.2775212  0.0161166  17.220  < 2e-16 ***\ny12         1.0462912  0.0161407  64.823  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.0253 on 47 degrees of freedom\nMultiple R-squared:  0.9959,    Adjusted R-squared:  0.9948 \nF-statistic: 949.2 on 12 and 47 DF,  p-value: < 2.2e-16\n\n\n\n\nA matrix: 12 × 11 of type dbl\n\n    23456789101112\n\n\n    100000000000\n    210000000000\n    301000000000\n    400100000000\n    500010000000\n    600001000000\n    700000100000\n    800000010000\n    900000001000\n    1000000000100\n    1100000000010\n    1200000000001\n\n\n\n\n\n해석 : 따라서 coefficients 보면 y1없는데 \\(\\beta_1\\) = 0이라 여기 안 나온거임, 그래서 여기서는 Intercept값이 y1(1월)값임(즉, Intercept값이 \\(\\beta_0\\)값). 해석하면 1월이 기준점이 되는거기에 y2는 2월과 1월의 차이를 의미, y3는 3월과 1월의 차이 y2 맨 뒤 보면 별표3개 안 붙어 있는데 이거는 밑에 신뢰성 유의하지 않다는 의미(0.001만큼), 이유가 1월과 2월의 차이는 별로 의미없다는 것이라서 contrasts를 보면 1월이 다 0인 것을 볼 수 있는데, 이를 보아 1월이 기준점임을 알 수 있음.\n\n\n\\(\\sum \\beta_i\\) = 0 가정\n\n\nreg3 <- lm(lndep ~ trend + y, data = tmp.data, contrasts = list(y = \"contr.sum\"))\nsummary(reg3)\n\n\nCall:\nlm(formula = lndep ~ trend + y, data = tmp.data, contrasts = list(y = \"contr.sum\"))\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.038679 -0.018689 -0.001468  0.015185  0.057288 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.3260849  0.0067177 941.703  < 2e-16 ***\ntrend        0.0106603  0.0001925  55.388  < 2e-16 ***\ny1          -0.2618944  0.0108845 -24.061  < 2e-16 ***\ny2          -0.2452853  0.0108675 -22.571  < 2e-16 ***\ny3           0.0550335  0.0108538   5.070 6.63e-06 ***\ny4          -0.0307393  0.0108436  -2.835  0.00674 ** \ny5          -0.1128456  0.0108368 -10.413 8.53e-14 ***\ny6          -0.1063078  0.0108334  -9.813 5.87e-13 ***\ny7           0.2624217  0.0108334  24.223  < 2e-16 ***\ny8          -0.1418018  0.0108368 -13.085  < 2e-16 ***\ny9          -0.2259700  0.0108436 -20.839  < 2e-16 ***\ny10          0.0073656  0.0108538   0.679  0.50071    \ny11          0.0156268  0.0108675   1.438  0.15708    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.0253 on 47 degrees of freedom\nMultiple R-squared:  0.9959,    Adjusted R-squared:  0.9948 \nF-statistic: 949.2 on 12 and 47 DF,  p-value: < 2.2e-16\n\n\n\n해석 : 이번에는 y12가 빠졌는데, 위에서 \\(\\sum \\beta_i\\) = 0라고 했는데 이는 자연스레 \\(\\beta_1 + \\beta_2 + \\beta_3 ... + \\beta_{12}\\) = 0임을 의미한다. 다시 말하면, \\(\\beta_{12} = -(\\beta_1 + \\beta_2 + \\beta_3 ... \\beta_{11})\\)라는 말이 된다. Intercept(6.3260849) 전체 평균을 의미(월별 상관없이) 즉, 1월은 전체 평균(6.3260849) 대비 -0.2618944한 거 만큼 의미 12월은 안나왔는데 12월은 위의 식처럼 평균대비 1~11을 다 빼면 나온다. 즉 ,0.7843966 + 6.3260849 = 7.1104815 -> 12월 거\n\n- coef 1~11 더한 거\n\nc(-0.2618944,-0.2452853,0.0550335,-0.0307393,-0.1128456,-0.1063078, 0.2624217,-0.1418018 ,-0.2259700,0.0073656,0.0156268) %>% sum\n\n-0.7843966\n\n\n\ntmp.data[, fitted_lndep := fitted(reg)]\n\n\nggplot(tmp.data, aes(day, lndep)) + \ngeom_line(col = 'skyblue', lwd = 1) +\ngeom_line(aes(day, fitted_lndep), col = 'orange', lwd = 1) +\nscale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\nlabs(title = \"department sales after log transformation vs estimated value\") +\ntheme_bw() +\ntheme(axis.title = element_blank())\n\n\n\n\n\n파랑색이 원래, 노랑색이 적합된 값 의미\n\n\n\n\n\ntmp.data[, res := resid(reg)]\n\n\nggplot(tmp.data, aes(day, res)) + \ngeom_line(col = 'skyblue') +\ngeom_point(col = 'steelblue') +\nscale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\ngeom_hline(yintercept = 0, col = 'grey', lty = 2) +\nlabs(title = \"Time series plot of residuals\") +\ntheme_bw() +\ntheme(axis.title = element_blank())\n\n\n\n\n\n그림보면 양의 상관관계가 있어보임. 그래서 D.W test해보면\n\n\ndwtest(reg, alternative = 'two.sided')\n\n\n    Durbin-Watson test\n\ndata:  reg\nDW = 0.82642, p-value = 4.781e-06\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n결과 독립성 확보 실패, 정확하게 1차 자기 상관관계가 있다. 특히 1차 양의 자기상관관계\n\n\n\n\n\n\ntmp.data_sub <- tmp.data[,.(lndep, trend)]\ntmp.data_sub %>% head\n\n\n\nA data.table: 6 × 2\n\n    lndeptrend\n    <dbl><int>\n\n\n    6.0473721\n    6.1268692\n    6.4085293\n    6.3350544\n    6.2841345\n    6.2841346\n\n\n\n\n- 데이터 생성\n\n아래 12, 6, 4 .. 은 주기 의미\n\n\ntmp.data_sub <- cbind(tmp.data_sub,\n                      tmp.data_sub[, lapply(as.list(1:5),\n                                            function(i) sin(2*pi*i/12*trend))])\n                                            \nnames(tmp.data_sub)[-(1:2)] <- paste(\"sin\", c(12, 6, 4, 3, 2.4), sep = \"_\")\n\ntmp.data_sub <- cbind(tmp.data_sub,\n                      tmp.data_sub[, lapply(as.list(1:5),\n                                            function(i) cos(2*pi*i/12*trend))])\n                                            \nnames(tmp.data_sub)[-(1:7)] <- paste(\"cos\", c(12, 6, 4, 3, 2.4), sep = \"_\")\n                                            \ntmp.data_sub %>% head                                            \n\n\n\nA data.table: 6 × 12\n\n    lndeptrendsin_12sin_6sin_4sin_3sin_2.4cos_12cos_6cos_4cos_3cos_2.4\n    <dbl><int><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    6.04737215.000000e-01 8.660254e-01 1.000000e+00 8.660254e-01 5.000000e-01 8.660254e-01 0.5 6.123234e-17-0.5-8.660254e-01\n    6.12686928.660254e-01 8.660254e-01 1.224647e-16-8.660254e-01-8.660254e-01 5.000000e-01-0.5-1.000000e+00-0.5 5.000000e-01\n    6.40852931.000000e+00 1.224647e-16-1.000000e+00-2.449294e-16 1.000000e+00 6.123234e-17-1.0-1.836970e-16 1.0 3.061617e-16\n    6.33505448.660254e-01-8.660254e-01-2.449294e-16 8.660254e-01-8.660254e-01-5.000000e-01-0.5 1.000000e+00-0.5-5.000000e-01\n    6.28413455.000000e-01-8.660254e-01 1.000000e+00-8.660254e-01 5.000000e-01-8.660254e-01 0.5 3.061617e-16-0.5 8.660254e-01\n    6.28413461.224647e-16-2.449294e-16 3.673940e-16-4.898587e-16 6.123234e-16-1.000000e+00 1.0-1.000000e+00 1.0-1.000000e+00\n\n\n\n\n\n\n\nreg_2 <- lm(lndep ~., data = tmp.data_sub)\nsummary(reg_2)\n\n\nCall:\nlm(formula = lndep ~ ., data = tmp.data_sub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.08232 -0.04855  0.00972  0.04645  0.08527 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.3237250  0.0148062 427.100  < 2e-16 ***\ntrend        0.0107376  0.0004242  25.315  < 2e-16 ***\nsin_12      -0.0277129  0.0103066  -2.689 0.009829 ** \nsin_6       -0.0382551  0.0102107  -3.747 0.000481 ***\nsin_4       -0.1555546  0.0101931 -15.261  < 2e-16 ***\nsin_3        0.0666506  0.0101872   6.543 3.70e-08 ***\nsin_2.4      0.0128922  0.0101849   1.266 0.211691    \ncos_12       0.0857900  0.0101931   8.416 5.21e-11 ***\ncos_6        0.1675743  0.0101931  16.440  < 2e-16 ***\ncos_4        0.1592698  0.0101931  15.625  < 2e-16 ***\ncos_3        0.1267107  0.0101931  12.431  < 2e-16 ***\ncos_2.4      0.2000603  0.0101931  19.627  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.05578 on 48 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9749 \nF-statistic: 209.5 on 11 and 48 DF,  p-value: < 2.2e-16\n\n\n\n해석 : sin_2.4 하나 유의하지 않다고 나옴(별표) 원래 sin, cos함수는 1부터 -1까지 갖는데 Estimate 나온 숫자만큼 진폭을 줄여주거나 늘려줌.\n\n\ntmp.data_sub[, day := tmp.data$day]\ntmp.data_sub[, fitted_lndep := fitted(reg_2)]\n\n\nggplot(tmp.data_sub, aes(day, lndep)) + \ngeom_line(col = 'skyblue', lwd = 1) +\ngeom_line(aes(day, fitted_lndep), col = 'orange', lwd = 1) +\nscale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\ntheme_bw() +\ntheme(axis.title = element_blank())\n\n\n\n\n\n\n\n\ntmp.data_sub[, res := resid(reg_2)]\nggplot(tmp.data_sub, aes(day, res)) +\ngeom_line(col = 'skyblue') +\ngeom_point(col = 'steelblue') +\nscale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\ntheme_bw() +\ntheme(axis.title = element_blank())\n\n\n\n\n\n해석 : 1차 음의 자기상관관계가 있다. 다른 것보다 등분산성의 문제가 있어보인다.(그저 보기에) = 독립성문제\n\n\n\n\n\ndwtest(reg_2)\ndwtest(reg_2, alternative = \"two.side\")\ndwtest(reg_2, alternative = \"less\")\n\n\n    Durbin-Watson test\n\ndata:  reg_2\nDW = 3.2703, p-value = 1\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n    Durbin-Watson test\n\ndata:  reg_2\nDW = 3.2703, p-value = 1.269e-06\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n    Durbin-Watson test\n\ndata:  reg_2\nDW = 3.2703, p-value = 6.346e-07\nalternative hypothesis: true autocorrelation is less than 0\n\n\n\nalternative 안 쓰면 default는 greater다 첫 번째 4에가까운 값이 나오면 two.side, less 둘 다 해보는게 좋음. two.side 결과보면 0이 아니다. -> 기각 less 결과보면 ’음의 상관관계가 있다’의 결과 p-value가 매우 작아 기각할 수 있다."
  },
  {
    "objectID": "posts/time-series/2022-10-02-시계열자료분석-학습1.html",
    "href": "posts/time-series/2022-10-02-시계열자료분석-학습1.html",
    "title": "(수업) 시계열 자료분석 실습 1",
    "section": "",
    "text": "불규칙, 추세성분, 주기성분의 이해\n\nlibrary('data.table')\nlibrary('tidyverse')\nlibrary('gridExtra') # grid.arrange 사용해주는(그림 한 화면에 그려주는, 사실 jupyter에서는 별로 쓸모 없음) \nlibrary('lmtest') # dwtest\n\n\n\n\nset.seed(1245)\nn = 100\n\n\nset.seed(1245)\nz <- 5000 + 20*rnorm(n)\nz %>% head\n\n\n5008.382801322284970.883327897544970.882398360754979.687072561944989.338017215614986.32787849186\n\n\n- 같은 방법\n\nset.seed(1245)\nrnorm(n, 5000, 20) %>% head\n\n\n5008.382801322284970.883327897544970.882398360754979.687072561944989.338017215614986.32787849186\n\n\n\nplot(rnorm(n), type = 'l')\nabline(h = 0, lty = 2)\n\n\n\n\n\n20곱하면 표준편차가 +- 20정도 증가\n\n\nplot(20*rnorm(n), type = 'l')\nabline(h = 0, lty = 2)\n\n\n\n\n\n5000더하면 평균도 5000증가\n\n\nplot(5000 + 20*rnorm(n), type = 'l')\nabline(h = 5000, lty = 2)\n\n\n\n\nts : TimeSeries로 바꿔주는 함수(시계열)\n\nz.ts <- ts(z,\n           start = c(1980,1), # 1980년 1월부터 시작\n           frequency = 12) # 12면 월별로, 4면 분기별로, 365면 일별로\nz.ts\n\n\n\nA Time Series: 9 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    19805008.3834970.8834970.8824979.6874989.3384986.3285006.5185018.4514998.9835005.9444988.9784985.049\n    19815017.0354999.1034970.2894994.1244962.2044986.3115013.9224970.4944988.5714978.3634974.5295009.565\n    19824976.2174996.0124966.7555029.4895018.5755011.3985013.2095023.4214997.9015012.8204982.2455003.137\n    19835002.3525027.8035009.0284978.8744992.8705006.6864995.8024985.5614984.2865000.3535002.7724989.359\n    19845030.0114985.2724999.7675020.0354974.9765043.3494966.2285003.0274976.1265030.5124983.9015016.143\n    19854966.1775044.2414999.5144995.4454992.8184985.3865009.7784980.8184979.3394983.1665010.1625000.873\n    19864979.7124973.1134994.0885004.4155024.6815001.0015019.1754964.4174994.1124971.1705021.8705015.610\n    19874996.4874984.2194967.2795008.6345032.1914992.3095005.2094999.6485025.7374996.7664985.4084994.681\n    19885013.3425027.1085012.3205010.116                                                                \n\n\n\n\n\nz.ts %>% cycle # 주기알려줌\n\n\n\nA Time Series: 9 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    1980 1 2 3 4 5 6 7 8 9101112\n    1981 1 2 3 4 5 6 7 8 9101112\n    1982 1 2 3 4 5 6 7 8 9101112\n    1983 1 2 3 4 5 6 7 8 9101112\n    1984 1 2 3 4 5 6 7 8 9101112\n    1985 1 2 3 4 5 6 7 8 9101112\n    1986 1 2 3 4 5 6 7 8 9101112\n    1987 1 2 3 4 5 6 7 8 9101112\n    1988 1 2 3 4                \n\n\n\n\n\nz.ts %>% time\n\n\n\nA Time Series: 9 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    19801980.0001980.0831980.1671980.2501980.3331980.4171980.5001980.5831980.6671980.7501980.8331980.917\n    19811981.0001981.0831981.1671981.2501981.3331981.4171981.5001981.5831981.6671981.7501981.8331981.917\n    19821982.0001982.0831982.1671982.2501982.3331982.4171982.5001982.5831982.6671982.7501982.8331982.917\n    19831983.0001983.0831983.1671983.2501983.3331983.4171983.5001983.5831983.6671983.7501983.8331983.917\n    19841984.0001984.0831984.1671984.2501984.3331984.4171984.5001984.5831984.6671984.7501984.8331984.917\n    19851985.0001985.0831985.1671985.2501985.3331985.4171985.5001985.5831985.6671985.7501985.8331985.917\n    19861986.0001986.0831986.1671986.2501986.3331986.4171986.5001986.5831986.6671986.7501986.8331986.917\n    19871987.0001987.0831987.1671987.2501987.3331987.4171987.5001987.5831987.6671987.7501987.8331987.917\n    19881988.0001988.0831988.1671988.250                                                                \n\n\n\n\n\nz.ts %>% frequency\n\n12\n\n\n\nz.ts %>% start\n\n\n19801\n\n\n\nz.ts %>% end\n\n\n19884\n\n\n\nz.ts %>% tsp # 시작, 끝, 주기 알려줌\n\n\n19801988.2512\n\n\n\nts장점 : 기본적으로 linetype = l로 해줌, 부가 옵션들이 더 나옴\n\n\nts.plot(z.ts, xlab = \"date\", ylab = \"zt\",\n       main =\"irregular elements\")\nabline(h = 5000)\n\n\n\n\n- 예제)\n\na_ <- \"1980/1/1\"\na_ %>% class\nas.Date(a_) %>% class\n\n'character'\n\n\n'Date'\n\n\n\n보이기에 비슷해 보이는데 명확하게 분류되는게 작동면에서 유리\n\n\ntmp.data <- data.table(Time = seq.Date(as.Date(\"1980/1/1\"),\n                                       by = \"month\",\n                                       length.out = 100),\n                       z = z)\ntmp.data %>% head\n\n\n\nA data.table: 6 × 2\n\n    Timez\n    <date><dbl>\n\n\n    1980-01-015008.383\n    1980-02-014970.883\n    1980-03-014970.882\n    1980-04-014979.687\n    1980-05-014989.338\n    1980-06-014986.328\n\n\n\n\n\nggplot(tmp.data, aes(Time, z)) +\ngeom_line(col = 'steelblue') +\ngeom_hline(yintercept = 5000, col = 'grey80', lty = 2) + # 선 긋기\nggtitle(\"irregular elements\") +\nscale_x_date(date_breaks = \"year\", date_labels = \"%Y\") + # 연 단위로 보이게 만들기\ntheme_bw() +\ntheme(text = element_text(size = 16), # 글씨 크기 조절 옵션\n      axis.title = element_blank()) # x, y label 제거\n\n\n\n\n\n\n\n\n하는 방법 : 일단 추세를 하나 그리고 노이즈를 더해준다.\n\n\nset.seed(1234)\nn = 100\nt <- 1:n\nx <- 0.5*t # 추세\nz <- 0.5*t + rnorm(n) # 추세 + 오차\n\n\nplot(x, type = 'l')\n\n\n\n\n- 기본 추세 + 노이즈\n\nplot(z, type = 'l')\n\n\n\n\n- 노이즈 추가, 연도, 주기, 변경\n\nz.ts <- ts(z, start = c(1980, 1), frequency = 12)\nx.ts <- ts(x, start = c(1980, 1), frequency = 12)\n\n- 추세선 + (추세 + 노이즈)선\n\nts.plot(z.ts, x.ts)\n\n\n\n\n- 추세선, 추세 + 오차선 색 다르게해서 그리기\n\nts.plot(z.ts, x.ts,\n        col = c('blue', 'red'),\n        lty = 1:2,\n        xlab = \"date\",\n        ylab = \"zt\",\n        main = \"trend component\")\n\nlegend(\"topleft\", # 범례 추가\n       legend = c(\"series\", \"trend\"),\n       lty = 1:2,\n       col = c(\"blue\", \"red\"))\n\n\n\n\n\n\n\n\nn = 120 # 계절성분 120개\nt <- 1:n\na <- rnorm(n,0,1) # 오차\n\n\nplot(a, type = 'l')\n\n\n\n\n\n앞에 0.8곱해주면 분산을 줄여줌(오차를 줄이니까) 10더한 것은 평균 10증가 -> 불규칙성분만들기\n\n\nplot(10 + 0.8*a, type = 'l')\n\n\n\n\n- 주기가 있는 사인함수\n\nplot(sin((2*pi*t)/12),type = 'l')\n\n\n\n\n- z는 위의 sin함수에 노이즈가 더해진 형태\n\nz <- 10 + 3*sin((2*pi*t)/12) + 0.8*a\n\n\nsin함수에 오차가 섞여서 계절성을 띄는 성분이 됨.\n\n\nz.ts <- ts(z,\n           start = c(1985, 1),\n           frequency = 12)\nplot(z.ts,\n     xlab = \"date\",\n     ylab = \"zt\",\n     main = \"seasonal component\")\n\n\n\n\n\n\n\n\n\n\nx <- seq(0, 48, 0.01)\ns <- 12\npar(mfrow = c(4, 1))\n\n\nplot(x, sin(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2)\n\nplot(x, cos(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('cos::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2)\n\n\n\n\n\n\n\n\nsin, cos 두 함수 더한 함수 아래는 각각 weight를 다르게 해 반영한 함수\n\n\nplot(x, sin(2*pi*x/s) + cos(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin+cos::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2)\nabline(v = seq(1.5, 48, by = s), lty = 2)\n\nplot(x, 1.5*sin(2*pi*x/s) + 0.7*cos(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin+cos::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = s), lty = 2)\nabline(v = seq(1.5, 24, by = s), lty = 2)\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(4, 1))\n\n\ns <- 12\nplot(x, sin(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2)\n\ns <- 6\nplot(x, sin(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2)\n\n\n\n\n\n\n\n\n주기는 6그대로인데 weight를 다르게하면 또 다른 다양한 형태가 나온다.\n\n\nplot(x, sin(2*pi*x/12) + sin(2*pi*x/6), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2) \n\nplot(x, 2*sin(2*pi*x/12) + 0.8*sin(2*pi*x/6), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 48, by = s), lty = 2) \n\n\n\n\n\n\n\n\n\n\ns <- 12\nplot(x, sin(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = s), lty = 2)\n\ns <- 6\nplot(x, sin(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = s), lty = 2)\n\ns <- 3\nplot(x, sin(2*pi*x/s), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin::', \"frequncy=\", s))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = s), lty = 2)\n\nplot(x, sin(2*pi*x/12) + sin(2*pi*x/6) + sin(2*pi*x/3), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin + sin + sin::', \"frequncy=\", 12))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = s), lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n여러개 섞으면서 그리기 마지막 거는 추세까지 섞임\n\n\nplot(x, sin(2*pi*x/12) + sin(2*pi*x/6) + sin(2*pi*x/3), type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = paste0('sin + sin + sin::', \"frequncy=\", 12))\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = 12), lty = 2)\n\ny <- sin(2*pi*x/12) + sin(2*pi*x/6) + sin(2*pi*x/3) + cos(2*pi*x/12) + cos(2*pi*x/6) + cos(2*pi*x/3)\n\nplot(x, y, type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = \"s+s+s+c+c+c : frequncy=, 12\")\nabline(h = 0, lty = 2)\nabline(v = seq(0, 24, by = 12), lty = 2)\n\ny2 <- x*0.5 + sin(2*pi*x/12) + sin(2*pi*x/6) +sin(2*pi*x/3) + cos(2*pi*x/12) + cos(2*pi*x/6) + cos(2*pi*x/3)\n\nplot(x, y2, type = 'l', col = 'steelblue', lwd = 2,\n     xlab = \"\", ylab = \"\", main = \"s+s+s+c+c+c frequency=12\")\nabline(a = 0, b = 0.5, lty = 2)\nabline(v = seq(0, 24, by = 12), lty = 2)\n\n\n\n\n\n\n\n\n\n\n\nn <- 100\nt <- 1:n\na1 <- -0.8 # 진폭\na2 <- 1.4 # 진폭\nphi1 <- pi/3\nphi2 <- 3*pi/4\nfirst <- a1*sin(pi*t/6 + phi1) # 첫 번째 주기성분(주기 6)\nsecond <- a2*sin(pi*t/3 + phi2) # 두 번째 주기성분(주기 3)\n\n\ndt <- data.table(t = t,\n                 first = first, # 첫 번째 주기 성분\n                 second = second,# 두 번째 주기 성분\n                 z = first + second)# 그 두 개 더한 성분\n\np1 <- ggplot(dt, aes(t, first)) + geom_line(col = 'skyblue', size = 1) +\ngeom_point(col = 'steelblue', size = 1) +\nylim(-2.5, 2) + xlab(\"\") +\nscale_x_continuous(breaks = seq(1, 100, by = 12)) +\ntheme_bw()\n\np2 <- ggplot(dt, aes(t, second)) + geom_line(col = 'skyblue', size = 1) +\ngeom_point(col = 'steelblue', size = 1) +\nylim(-2.5, 2) + xlab(\"\") +\nscale_x_continuous(breaks = seq(1, 100, by = 12)) +\ntheme_bw()\n\np3 <- ggplot(dt, aes(t, z)) + geom_line(col = 'skyblue', size = 1) +\ngeom_point(col = 'steelblue', size = 1) +\nylim(-2.5, 2) + xlab(\"\") +\nscale_x_continuous(breaks = seq(1, 100, by = 12)) +\ntheme_bw()\n\n\np1\np2\np3\n\n\n\n\n\n\n\n\n\n\n\ngrid.arrange(p1, p2, p3, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nz <- scan(\"depart.txt\")\n\ndep <- ts(z, frequency = 12, start = c(1984, 1)) # TimeSeries로 바꿔도되고 안해도 되는데 여기서는 함\nplot(dep)\n\n\n\n\n\n매출액이 증가하는 추세가 보임 계졀성도 있어보임(특히 1년주기) 또 점점 분산이 증가하는 이분산성을 보이는데 따라서 로그 변환을 해준다.\n\n\ntmp.data <- data.table(\n    day = seq.Date(as.Date(\"1984-01-01\"),\n                   by = 'month', length.out = length(z)),\n    z = z\n    )\n\ntmp.data[, lndep := log(z)] # 로그변환\ntmp.data[, y := as.factor(as.integer(cycle(dep)))] # factor씌우는 게 그냥 넣으면 그대로 안 받아들여져서\ntmp.data[, trend := 1:length(z)]\n\ntmp.data %>% head\n\n\n\nA data.table: 6 × 5\n\n    dayzlndepytrend\n    <date><dbl><dbl><fct><int>\n\n\n    1984-01-014236.04737211\n    1984-02-014586.12686922\n    1984-03-016076.40852933\n    1984-04-015646.33505444\n    1984-05-015366.28413455\n    1984-06-015366.28413466\n\n\n\n\n\np1 <- ggplot(tmp.data, aes(day, z)) + \ngeom_line(col = 'skyblue') +\ngeom_point(col = 'steelblue') +\nscale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\nlabs(title = \"monthly department store sales TimeSeries plot\") +\ntheme_bw() +\ntheme(axis.title = element_blank())\n\np2 <- ggplot(tmp.data, aes(day, lndep)) + \ngeom_line(col = 'skyblue') +\ngeom_point(col = 'steelblue') +\nscale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\nlabs(title = \"monthly department store sales TimeSeries plot after log transformation\") +\ntheme_bw() +\ntheme(axis.title = element_blank())\n\n\n로그 변환 이후 분산 폭이 줄어듦을 볼 수 있다.\n\n\np1\np2 # 로그변환한 거\n\n\n\n\n\n\n\n\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n지시함수(Indicater)를 사용하고 싶다면, \\(\\beta_0\\) = 0 or \\(\\sum \\beta_i\\) = 0 or \\(\\beta_1\\) = 0 셋 중 하나를 가정해야함. 아래는 \\(\\beta_0\\) = 0 가정\n\n\ntmp.data %>% head\n\n\n\nA data.table: 6 × 5\n\n    dayzlndepytrend\n    <date><dbl><dbl><fct><int>\n\n\n    1984-01-014236.04737211\n    1984-02-014586.12686922\n    1984-03-016076.40852933\n    1984-04-015646.33505444\n    1984-05-015366.28413455\n    1984-06-015366.28413466\n\n\n\n\n\nreg <- lm(lndep ~ 0 + trend + y, data = tmp.data)\nsummary(reg)\n\n\nCall:\nlm(formula = lndep ~ 0 + trend + y, data = tmp.data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.038679 -0.018689 -0.001468  0.015185  0.057288 \n\nCoefficients:\n       Estimate Std. Error t value Pr(>|t|)    \ntrend 0.0106603  0.0001925   55.39   <2e-16 ***\ny1    6.0641904  0.0122952  493.21   <2e-16 ***\ny2    6.0807995  0.0123718  491.50   <2e-16 ***\ny3    6.3811183  0.0124509  512.50   <2e-16 ***\ny4    6.2953455  0.0125325  502.32   <2e-16 ***\ny5    6.2132392  0.0126164  492.47   <2e-16 ***\ny6    6.2197771  0.0127027  489.64   <2e-16 ***\ny7    6.5885065  0.0127914  515.08   <2e-16 ***\ny8    6.1842831  0.0128823  480.06   <2e-16 ***\ny9    6.1001148  0.0129754  470.13   <2e-16 ***\ny10   6.3334505  0.0130707  484.56   <2e-16 ***\ny11   6.3417116  0.0131681  481.60   <2e-16 ***\ny12   7.1104816  0.0132676  535.93   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.0253 on 47 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 3.199e+05 on 13 and 47 DF,  p-value: < 2.2e-16\n\n\n\n해석 : coefficients에서 trend 보면 매월 0.0106603씩 증가함을 볼 수 있음. 로그 취한 것이기에 6.0641904, 6.0807995등은 각각 로그매출액의 1월의 평균, 2월의 평균을 의미(쭉쭉 12월까지) 미국은 블랙 프라이데이등으로 인해 12월 매출이 높은 것을 볼 수 있음, 또 높은 구간은 여름(7월)이고 이는 그래프에서도 나타남.\n\n\n\\(\\beta_1\\) = 0 가정 위 처럼 0 안넣으면 자동으로 “\\(\\beta_1\\) = 0 가정” 들어간다.\n\n\nreg2 <- lm(lndep ~ trend + y, data = tmp.data)\nsummary(reg2)\ncontrasts(tmp.data$y)\n\n\nCall:\nlm(formula = lndep ~ trend + y, data = tmp.data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.038679 -0.018689 -0.001468  0.015185  0.057288 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 6.0641904  0.0122952 493.215  < 2e-16 ***\ntrend       0.0106603  0.0001925  55.388  < 2e-16 ***\ny2          0.0166091  0.0160024   1.038   0.3046    \ny3          0.3169279  0.0160059  19.801  < 2e-16 ***\ny4          0.2311551  0.0160117  14.437  < 2e-16 ***\ny5          0.1490488  0.0160198   9.304 3.12e-12 ***\ny6          0.1555867  0.0160302   9.706 8.32e-13 ***\ny7          0.5243161  0.0160429  32.682  < 2e-16 ***\ny8          0.1200927  0.0160579   7.479 1.54e-09 ***\ny9          0.0359244  0.0160752   2.235   0.0302 *  \ny10         0.2692601  0.0160948  16.730  < 2e-16 ***\ny11         0.2775212  0.0161166  17.220  < 2e-16 ***\ny12         1.0462912  0.0161407  64.823  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.0253 on 47 degrees of freedom\nMultiple R-squared:  0.9959,    Adjusted R-squared:  0.9948 \nF-statistic: 949.2 on 12 and 47 DF,  p-value: < 2.2e-16\n\n\n\n\nA matrix: 12 × 11 of type dbl\n\n    23456789101112\n\n\n    100000000000\n    210000000000\n    301000000000\n    400100000000\n    500010000000\n    600001000000\n    700000100000\n    800000010000\n    900000001000\n    1000000000100\n    1100000000010\n    1200000000001\n\n\n\n\n\n해석 : 따라서 coefficients 보면 y1없는데 \\(\\beta_1\\) = 0이라 여기 안 나온거임, 그래서 여기서는 Intercept값이 y1(1월)값임(즉, Intercept값이 \\(\\beta_0\\)값). 해석하면 1월이 기준점이 되는거기에 y2는 2월과 1월의 차이를 의미, y3는 3월과 1월의 차이 y2 맨 뒤 보면 별표3개 안 붙어 있는데 이거는 밑에 신뢰성 유의하지 않다는 의미(0.001만큼), 이유가 1월과 2월의 차이는 별로 의미없다는 것이라서 contrasts를 보면 1월이 다 0인 것을 볼 수 있는데, 이를 보아 1월이 기준점임을 알 수 있음.\n\n\n\\(\\sum \\beta_i\\) = 0 가정\n\n\nreg3 <- lm(lndep ~ trend + y, data = tmp.data, contrasts = list(y = \"contr.sum\"))\nsummary(reg3)\n\n\nCall:\nlm(formula = lndep ~ trend + y, data = tmp.data, contrasts = list(y = \"contr.sum\"))\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.038679 -0.018689 -0.001468  0.015185  0.057288 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.3260849  0.0067177 941.703  < 2e-16 ***\ntrend        0.0106603  0.0001925  55.388  < 2e-16 ***\ny1          -0.2618944  0.0108845 -24.061  < 2e-16 ***\ny2          -0.2452853  0.0108675 -22.571  < 2e-16 ***\ny3           0.0550335  0.0108538   5.070 6.63e-06 ***\ny4          -0.0307393  0.0108436  -2.835  0.00674 ** \ny5          -0.1128456  0.0108368 -10.413 8.53e-14 ***\ny6          -0.1063078  0.0108334  -9.813 5.87e-13 ***\ny7           0.2624217  0.0108334  24.223  < 2e-16 ***\ny8          -0.1418018  0.0108368 -13.085  < 2e-16 ***\ny9          -0.2259700  0.0108436 -20.839  < 2e-16 ***\ny10          0.0073656  0.0108538   0.679  0.50071    \ny11          0.0156268  0.0108675   1.438  0.15708    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.0253 on 47 degrees of freedom\nMultiple R-squared:  0.9959,    Adjusted R-squared:  0.9948 \nF-statistic: 949.2 on 12 and 47 DF,  p-value: < 2.2e-16\n\n\n\n해석 : 이번에는 y12가 빠졌는데, 위에서 \\(\\sum \\beta_i\\) = 0라고 했는데 이는 자연스레 \\(\\beta_1 + \\beta_2 + \\beta_3 ... + \\beta_{12}\\) = 0임을 의미한다. 다시 말하면, \\(\\beta_{12} = -(\\beta_1 + \\beta_2 + \\beta_3 ... \\beta_{11})\\)라는 말이 된다. Intercept(6.3260849) 전체 평균을 의미(월별 상관없이) 즉, 1월은 전체 평균(6.3260849) 대비 -0.2618944한 거 만큼 의미 12월은 안나왔는데 12월은 위의 식처럼 평균대비 1~11을 다 빼면 나온다. 즉 ,0.7843966 + 6.3260849 = 7.1104815 -> 12월 거\n\n- coef 1~11 더한 거\n\nc(-0.2618944,-0.2452853,0.0550335,-0.0307393,-0.1128456,-0.1063078, 0.2624217,-0.1418018 ,-0.2259700,0.0073656,0.0156268) %>% sum\n\n-0.7843966\n\n\n\ntmp.data[, fitted_lndep := fitted(reg)]\n\n\nggplot(tmp.data, aes(day, lndep)) + \ngeom_line(col = 'skyblue', lwd = 1) +\ngeom_line(aes(day, fitted_lndep), col = 'orange', lwd = 1) +\nscale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\nlabs(title = \"department sales after log transformation vs estimated value\") +\ntheme_bw() +\ntheme(axis.title = element_blank())\n\n\n\n\n\n파랑색이 원래, 노랑색이 적합된 값 의미\n\n\n\n\n\ntmp.data[, res := resid(reg)]\n\n\nggplot(tmp.data, aes(day, res)) + \ngeom_line(col = 'skyblue') +\ngeom_point(col = 'steelblue') +\nscale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\ngeom_hline(yintercept = 0, col = 'grey', lty = 2) +\nlabs(title = \"Time series plot of residuals\") +\ntheme_bw() +\ntheme(axis.title = element_blank())\n\n\n\n\n\n그림보면 양의 상관관계가 있어보임. 그래서 D.W test해보면\n\n\ndwtest(reg, alternative = 'two.sided')\n\n\n    Durbin-Watson test\n\ndata:  reg\nDW = 0.82642, p-value = 4.781e-06\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n결과 독립성 확보 실패, 정확하게 1차 자기 상관관계가 있다. 특히 1차 양의 자기상관관계\n\n\n\n\n\n\ntmp.data_sub <- tmp.data[,.(lndep, trend)]\ntmp.data_sub %>% head\n\n\n\nA data.table: 6 × 2\n\n    lndeptrend\n    <dbl><int>\n\n\n    6.0473721\n    6.1268692\n    6.4085293\n    6.3350544\n    6.2841345\n    6.2841346\n\n\n\n\n- 데이터 생성\n\n아래 12, 6, 4 .. 은 주기 의미\n\n\ntmp.data_sub <- cbind(tmp.data_sub,\n                      tmp.data_sub[, lapply(as.list(1:5),\n                                            function(i) sin(2*pi*i/12*trend))])\n                                            \nnames(tmp.data_sub)[-(1:2)] <- paste(\"sin\", c(12, 6, 4, 3, 2.4), sep = \"_\")\n\ntmp.data_sub <- cbind(tmp.data_sub,\n                      tmp.data_sub[, lapply(as.list(1:5),\n                                            function(i) cos(2*pi*i/12*trend))])\n                                            \nnames(tmp.data_sub)[-(1:7)] <- paste(\"cos\", c(12, 6, 4, 3, 2.4), sep = \"_\")\n                                            \ntmp.data_sub %>% head                                            \n\n\n\nA data.table: 6 × 12\n\n    lndeptrendsin_12sin_6sin_4sin_3sin_2.4cos_12cos_6cos_4cos_3cos_2.4\n    <dbl><int><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    6.04737215.000000e-01 8.660254e-01 1.000000e+00 8.660254e-01 5.000000e-01 8.660254e-01 0.5 6.123234e-17-0.5-8.660254e-01\n    6.12686928.660254e-01 8.660254e-01 1.224647e-16-8.660254e-01-8.660254e-01 5.000000e-01-0.5-1.000000e+00-0.5 5.000000e-01\n    6.40852931.000000e+00 1.224647e-16-1.000000e+00-2.449294e-16 1.000000e+00 6.123234e-17-1.0-1.836970e-16 1.0 3.061617e-16\n    6.33505448.660254e-01-8.660254e-01-2.449294e-16 8.660254e-01-8.660254e-01-5.000000e-01-0.5 1.000000e+00-0.5-5.000000e-01\n    6.28413455.000000e-01-8.660254e-01 1.000000e+00-8.660254e-01 5.000000e-01-8.660254e-01 0.5 3.061617e-16-0.5 8.660254e-01\n    6.28413461.224647e-16-2.449294e-16 3.673940e-16-4.898587e-16 6.123234e-16-1.000000e+00 1.0-1.000000e+00 1.0-1.000000e+00\n\n\n\n\n\n\n\nreg_2 <- lm(lndep ~., data = tmp.data_sub)\nsummary(reg_2)\n\n\nCall:\nlm(formula = lndep ~ ., data = tmp.data_sub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.08232 -0.04855  0.00972  0.04645  0.08527 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.3237250  0.0148062 427.100  < 2e-16 ***\ntrend        0.0107376  0.0004242  25.315  < 2e-16 ***\nsin_12      -0.0277129  0.0103066  -2.689 0.009829 ** \nsin_6       -0.0382551  0.0102107  -3.747 0.000481 ***\nsin_4       -0.1555546  0.0101931 -15.261  < 2e-16 ***\nsin_3        0.0666506  0.0101872   6.543 3.70e-08 ***\nsin_2.4      0.0128922  0.0101849   1.266 0.211691    \ncos_12       0.0857900  0.0101931   8.416 5.21e-11 ***\ncos_6        0.1675743  0.0101931  16.440  < 2e-16 ***\ncos_4        0.1592698  0.0101931  15.625  < 2e-16 ***\ncos_3        0.1267107  0.0101931  12.431  < 2e-16 ***\ncos_2.4      0.2000603  0.0101931  19.627  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.05578 on 48 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9749 \nF-statistic: 209.5 on 11 and 48 DF,  p-value: < 2.2e-16\n\n\n\n해석 : sin_2.4 하나 유의하지 않다고 나옴(별표) 원래 sin, cos함수는 1부터 -1까지 갖는데 Estimate 나온 숫자만큼 진폭을 줄여주거나 늘려줌.\n\n\ntmp.data_sub[, day := tmp.data$day]\ntmp.data_sub[, fitted_lndep := fitted(reg_2)]\n\n\nggplot(tmp.data_sub, aes(day, lndep)) + \ngeom_line(col = 'skyblue', lwd = 1) +\ngeom_line(aes(day, fitted_lndep), col = 'orange', lwd = 1) +\nscale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\ntheme_bw() +\ntheme(axis.title = element_blank())\n\n\n\n\n\n\n\n\ntmp.data_sub[, res := resid(reg_2)]\nggplot(tmp.data_sub, aes(day, res)) +\ngeom_line(col = 'skyblue') +\ngeom_point(col = 'steelblue') +\nscale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\ntheme_bw() +\ntheme(axis.title = element_blank())\n\n\n\n\n\n해석 : 1차 음의 자기상관관계가 있다. 다른 것보다 등분산성의 문제가 있어보인다.(그저 보기에) = 독립성문제\n\n\n\n\n\ndwtest(reg_2)\ndwtest(reg_2, alternative = \"two.side\")\ndwtest(reg_2, alternative = \"less\")\n\n\n    Durbin-Watson test\n\ndata:  reg_2\nDW = 3.2703, p-value = 1\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n    Durbin-Watson test\n\ndata:  reg_2\nDW = 3.2703, p-value = 1.269e-06\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n    Durbin-Watson test\n\ndata:  reg_2\nDW = 3.2703, p-value = 6.346e-07\nalternative hypothesis: true autocorrelation is less than 0\n\n\n\nalternative 안 쓰면 default는 greater다 첫 번째 4에가까운 값이 나오면 two.side, less 둘 다 해보는게 좋음. two.side 결과보면 0이 아니다. -> 기각 less 결과보면 ’음의 상관관계가 있다’의 결과 p-value가 매우 작아 기각할 수 있다."
  },
  {
    "objectID": "posts/regression/2022-08-10_회귀분석.html",
    "href": "posts/regression/2022-08-10_회귀분석.html",
    "title": "회귀분석",
    "section": "",
    "text": "배운 내용정리\n\n\nlibrary('tidyverse')\n\n\n\n\n\n\nex1 <- tribble(\n    ~x, ~y,\n    1, 150,\n    2, 160,\n    3, 170,\n    4, 150,\n    5, 140,\n    6, 160,\n    7, 190\n    )\n\n\nex1\n\n\n\nA tibble: 7 × 2\n\n    xy\n    <dbl><dbl>\n\n\n    1150\n    2160\n    3170\n    4150\n    5140\n    6160\n    7190\n\n\n\n\n\nhistplot\n\n\nex1$y %>% hist()\n\n\n\n\n\n히스토그램은 계급 구간의 수가 중요하다(보통 10개 내외 사용)\n\n\nboxplot\n\n\nex1$y %>% boxplot()\n\n\n\n\n맨 위아래 가로바 밖은 이상치(outlier)들 박스 맨 아래는 Q1, 가운데 선은 Q2(평균), 박스 맨 위는 Q3를 의미한다. 이 때 Q3-Q1 = h라고 하며 맨 위의 가로바, 맨 아래의 가로바는 각각 Q3 + 1.5h, Q1 - 1.5h인 지점이다.\n\n\n\n모수(parameter) : 모집단이나 변수의 통계적 특성을 어떤 수치로 표현한 것 모수의 예로는 모집단의 중심위치를 나타내는 평균, 중간값, 최빈값과 값들이 중심에서 퍼져있는 정도, 즉 산포도를 나타내는 범위, 편차, 표준편차, 분산등이 있다. 이들 모수의 값은 대부분의 경우 알려져 있지 않으므로 표본을 이용하여 이들 값을 추정하게 된다.  통계랑(statisic) : 모수에 대응하여 표본의 특성을 잘 나타내는 수치 통계량은 표본을 이용하여 계산되므로 각 모수에다 ’표본’을 붙여 통계량을 나타낸다.ex) 표본 평균, 표본 분산 두 개 이상의 변수들의 관계에 대한 측도로는 공분산과 상관계수 등이 사용된다.\n\n\n\n확률변수(random variable) : 분석대상인 변수들이 갖는 각 결과에 하나의 실수값을 대응시켜주는 함수 확률(random)의 의미 : 확률변수의 값은 실제로 관측하기 전에는 어떠한 값이 될지 알 수 없다 확률분포(probability distribution) : 확률변수의 모든 가능한 값에 확률을 대응시킨 것 확률분포는 이산확률변수의 경우 표, 그림, 수식으로 나타내어지고, 연속확률변수의 경우 그림, 수식으로 주어진다.\n\n\n\\(E[Y] = \\sum\\limits_{i=1}^d{y_if_Y(y_i)}\\)\n기댓값은 다음과 같이 정의 되는데 위의 의미는 이산확률변수 Y의 기댓값은 \\(f_y(y_i)\\)가 가중치로 주어지는 가중평균이라는 것이다.\n\n\n\n공분산(covariance)는 두 변수간의 선형관계를 나타내는 측도로 다음과 같이 정의된다.\n\\(cov(Y_1, Y_2) = E[(Y_1 - \\mu Y_1)(Y_2 - \\mu Y_2)]\\)\n\n\n\n\\(\\rho = \\frac{cov(Y_1, Y_2)}{\\sigma Y1, \\sigma Y2}\\)\n상관계수 \\(\\rho\\)는 다음과 같이 해석된다. > 1) \\(\\rho\\)는 변수의 종류나 특정단위에 관계없는 측도로 -1과 +1 사이의 값을 가지며, \\(\\rho\\)의 값이 +1에 가까울수록 강한 양의 상관관계를,-1에 가까울수록 강한 음의 상관관계를 나타내며, \\(\\rho\\)의 값이 0에 가까울수록 선형관계는 약해진다. 2) \\(Y_1\\)과 \\(Y_2\\)의 대응되는 모든 값들이 한 직선 상에 위치하면 \\(\\rho\\)의 값은 -1이나 +1의 값을 가진다. 3) 상관계수 \\(\\rho\\)는 단지 두 변수간의 선형관계만을 나타내는 측도이다. 그러므로 \\(\\rho\\) = 0인 경우에 두 변수의 선형상관관계는 없지만 다른 관계는 가질 수 있다.\n\n\n\n여러가지 이산확률분포들 중에서 회귀분석과 관련이 있는 분포는 이항분포(binomial distribution)와 포아송분포(poisson distribution)이다. 먼저 이항분포는 베르누이 시행(bernoulli trial)에 의해 정의되는데, 어느 실험이 1) 오직 두 가지의 가능한 결과만을 가지고, 2) 매 번 시행에서 성공의 확률이 같아야 하며, 3) 각 시행은 서로 독립이라는 세 가지 조건을 만족하면 이를 베르누이 시행이라 한다.\n동전던지기(앞 or 뒤), 품질검사(양품 or 불량품)가 이에 해당한다. 보통 일반적으로 베르누이 시행의 결과를 성공과 실패로 나타낸다.\n이항분포 : 베르누이 시행을 독립적으로 n번 반복할 때 나타나는 성공의 개수가 갖는 확률분포\n포아송분포 : 단위 시간당 또는 단위 영역당 발생하는 사건의 횟수를 나타내기 위한 분포\n예를들어, 어느 시간대에 가게에 찾아오는 고객의 수, 어느 도시에서 하루동안 발생하는 교통사고의 수 등은 포아송분포를 따른다. 사건발생 평균횟수가 \\(\\lambda\\)인 포아송분포를 따르는 확률변수 Y의 확률분포는 다음과 같이 주어진다.\n\\(f(y) = \\frac{e^{-\\lambda}\\lambda^y}{y!}\\)\n포아송분포의 특성은 평균과 분산이 모두 \\(\\lambda\\)로 같다.\n\n\n\n- 정규분포의 대표적인 성질 하나의 정규분포를 따르는 확률변수의 선형함수 역시 정규분포를 따른다. 즉, 확률변수 Y가 평균이 \\(\\mu\\)이고 분산이 \\(\\sigma^2\\)인 정규분포를 따를 때 선형함수 a + bY는 평균이 a + \\(b\\mu\\)이고 분산이 \\(b^2\\sigma^2\\)인 정규분포를 따른다. 이것 때문에 표준화가 가능함.\n정규분포는 통계학에서 가장 많이 사용되는 확률분포이다. 그러나 모순적인 것은 정규분포를 정확하게 따르는 변수는 현실적으로 없다는 것이다. 왜냐하면 정규분포는 우선(-\\(\\infty\\), \\(\\infty\\))에서 정의되어야하고, 모든 구간에서의 확률이 0보다 커야 한다는 조건이 주어지기 때문이다. 그러므로 이를테면 항상 양수값만을 갖는 학생들의 신장이 정규분포를 따른다고 가정하는 것은 엄격하게 보면 잘못된 것일 수 있다. 그렇지만 이 가정이 타당성을 갖는 이유는 정규분포에서는 (\\(\\mu -3\\sigma, \\mu +3\\sigma\\))정도의 구간에 99.74%라는 거의 모든 확률이 포함되어 있기 때문이다. 신장의 경우 표준편차 \\(\\sigma\\)가 아주 크지 않는 경우에는 음수부분은 거의 확률이 존재하지 않는 것으로 보아도 무방하게 된다.\n중심극한정리(central limit theorem) : 확률변수 \\(Y1, Y2, ..., Y_n\\)들이 평균이 \\(\\mu\\)이고 분산이 \\(\\sigma^2 < \\infty\\)인 확률분포를 따르며  서로 독립일 때, 표본의 크기 n이 적당히 크면 표본평균의 분포는 근사적으로 정규분포에 가까워진다."
  },
  {
    "objectID": "posts/time-series/2022-10-05-시계열자료분석-학습2.html",
    "href": "posts/time-series/2022-10-05-시계열자료분석-학습2.html",
    "title": "(수업) 시계열 자료분석 실습 2",
    "section": "",
    "text": "평활법(MA, 단순지수, 이중지수, Holt-Winter)\n\nlibrary('tidyverse')\nlibrary('forecast')\nlibrary('lmtest')\nlibrary('TTR')# SMA\nlibrary('data.table')\nlibrary('gridExtra')\n\n\nkings = scan(\"https://robjhyndman.com/tsdldata/misc/kings.dat\", skip = 3) # 영국 왕 수명 데이터\n\n\nkingstimeseries = ts(kings)\nplot.ts(kingstimeseries)\n\n\n\n\n\n\n\n윈도우 크기가 3인 Moving Averages 원래 데이터에 평활한 거 같이 그려짐\n\n\nkingstimeseriesSMA3 <- SMA(kingstimeseries, n = 3)\nplot.ts(kingstimeseries)\nlines(kingstimeseriesSMA3, col = 'red', lty = 2)\nlines(SMA(kingstimeseriesSMA3, n = 10), col = 'blue', lty = 2) # window 10개짜리\n\n\n\n\n\n\n\ntmp.dat <- data.table(kings = kings,\n                      t = 1:length(kings))\ntmp.dat[, sma3 := SMA(kingstimeseries, n = 3)]\ntmp.dat[, sma10 := SMA(kingstimeseries, n = 10)]\n\nggplot(tmp.dat, aes(t, kings)) + geom_line(col = 'skyblue') +\ngeom_point(col = 'steelblue') +\ntheme_bw() +\ntheme(axis.title.x = element_blank())\n\n\n\n\n\nmelt.tmp <- melt(tmp.dat, id = 't')\n\nggplot(melt.tmp, aes(t, value, col = variable, size = variable, lty = variable)) +\ngeom_line() +\ntheme_bw() +\nlabs(col = \"\") +\nscale_linetype_manual(values = c('solid', 'twodash', 'dashed')) +\nscale_color_manual(values = c('black', 'orange', 'steelblue')) +\nscale_size_manual(values = c(0.2, 1.2, 1.2)) +\nguides(lty = 'none', size = 'none') +\ntheme(axis.title.x = element_blank())\n\nWarning message:\n“Removed 11 row(s) containing missing values (geom_path).”\n\n\n\n\n\n\n모형을 평가하는 수치들 위의 3개는 window를 3개 가져갔을 때의 MSE들 아래 3개는 window를 10개 가져갔을 때의 MSE들\n\n\nmean((tmp.dat$kings- tmp.dat$sma3)^2, na.rm=T) ##MSE\nmean(abs(tmp.dat$kings- tmp.dat$sma3), na.rm=T) ##MAE\nmean(abs((tmp.dat$kings- tmp.dat$sma3)/tmp.dat$kings), na.rm=T)*100 ##MAPE\n\nmean((tmp.dat$kings- tmp.dat$sma10)^2, na.rm=T) ##MSE\nmean(abs(tmp.dat$kings- tmp.dat$sma10), na.rm=T) ##MAE\nmean(abs((tmp.dat$kings- tmp.dat$sma10)/tmp.dat$kings), na.rm=T)*100 ##MAPE\n\n128.752777777778\n\n\n9.025\n\n\n22.4739949863572\n\n\n229.22\n\n\n12.5272727272727\n\n\n31.8798030034434\n\n\n\n다른 자료\n\n\nz <- scan('mindex.txt')\nmindex <- ts(z, start = c(1986, 1), frequency = 12)\nmindex\n\n\n\nA Time Series: 9 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    1986 9.310.713.314.117.818.119.418.819.118.418.017.0\n    198719.520.119.415.715.616.114.916.014.618.318.223.0\n    198822.222.118.817.713.812.716.515.616.310.710.4 7.0\n    1989 4.7 4.5 4.0 6.0 6.2 5.7 4.4 4.2 5.0 5.8 6.4 4.9\n    1990 7.9 8.211.810.011.111.712.415.214.015.212.918.0\n    199114.412.7 8.311.511.911.610.3 8.511.612.314.511.1\n    199211.812.412.7 9.810.010.2 9.6 6.9 5.3 4.8 4.6 1.9\n    1993 3.8 4.7 7.7 7.0 7.2 7.8 8.611.410.711.811.316.0\n    199413.212.0 8.511.4                                \n\n\n\n\n\ntmp.dat <- data.table(ind = z,\n                      t = 1:length(z))\n\ntmp.dat[, sma3 := SMA(mindex, n = 3)]\ntmp.dat[, sma10 := SMA(mindex, n = 10)]\n\nggplot(tmp.dat, aes(t, ind)) + geom_line(col = 'skyblue', size = 1) +\ngeom_point(col = 'steelblue') +\ntheme_bw() +\ntheme(axis.title.x = element_blank())\n\n\n\n\n\nwindow 3개, 10개 나눠서 합쳐서 그리기\n\n\n\n\n\n\nz <- scan(\"mindex.txt\")\n\nmindex <- ts(z, start = c(1986, 1), frequency = 12)\n\ntmp.dat <- data.table(day = seq.Date(as.Date(\"1986-01-01\"), \n                                     by='month', \n                                     length.out=length(z)),\n                      ind = z)\nhead(tmp.dat)\n\nggplot(tmp.dat, aes(day, ind))+geom_line(col='skyblue') +\n  geom_point(col='steelblue')+\n  ggtitle(\"Intermediate shipment index\")+\n  theme_bw()+\n  theme(plot.title = element_text(size=20),\n        axis.title = element_blank())\n\n\n\nA data.table: 6 × 2\n\n    dayind\n    <date><dbl>\n\n\n    1986-01-01 9.3\n    1986-02-0110.7\n    1986-03-0113.3\n    1986-04-0114.1\n    1986-05-0117.8\n    1986-06-0118.1\n\n\n\n\n\n\n\n\n평균 = level이라 표현함 예를 들어) 상수평균모형같은 경우 level이 시간에 상관없이 일정하다. 단순지수평활에서는 level이 국지적으로 비슷하지만 전체적으로 보면 변화한다. alpha가 level을 평활할 때 쓰는 평활상수의미\n\n\n### 단순지수평활 alpha=0.9\n\n\nHoltWinters는 단순지수, 이중지수, 계절평활 3가지 다 가능 이번에는 단순지수평활이라 기울기, 계절성분을 안 쓸거라서 FALSE해줬음.\n\n\nfit0 <- HoltWinters(mindex, \n                    alpha = 0.9, # 레벨\n                    beta = FALSE, # 기울기에 대한 평활상수\n                    gamma = FALSE) # 계절성분에 대한 평활상수\nls(fit0) # 어떤 정보 값들 가지는 지 보여줌\n\n\n'alpha''beta''call''coefficients''fitted''gamma''seasonal''SSE''x'\n\n\n\n그 중에서 SSE 보고싶어서 SSE 선택\n\n\nfit0$SSE\n\n437.636076191925\n\n\n\nses는 단순지수 평활만 사용가능\n\n\nfit01 <- ses(mindex, \n             alpha = 0.9, # 사실 알파 안써도 얘가 적당히 넣어주긴함\n             initial = 'simple',\n             h = 10) # 앞으로 몇 개 더 예측해줄지 개수 설정\nls(fit01)\n\n\n'fitted''level''lower''mean''method''model''residuals''series''upper''x'\n\n\n\nsummary(fit01)\n\n\nForecast method: Simple exponential smoothing\n\nModel Information:\nSimple exponential smoothing \n\nCall:\n ses(y = mindex, h = 10, initial = \"simple\", alpha = 0.9) \n\n  Smoothing parameters:\n    alpha = 0.9 \n\n  Initial states:\n    l = 9.3 \n\n  sigma:  2.092\nError measures:\n                     ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.02051593 2.091975 1.614844 -2.575694 16.57215 0.2947652\n                    ACF1\nTraining set -0.01462494\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80       Lo 95    Hi 95\nMay 1994       11.14643 8.465459 13.82741  7.04623710 15.24663\nJun 1994       11.14643 7.539551 14.75332  5.63018345 16.66268\nJul 1994       11.14643 6.806897 15.48597  4.50968590 17.78318\nAug 1994       11.14643 6.181200 16.11167  3.55276359 18.74010\nSep 1994       11.14643 5.625970 16.66690  2.70361248 19.58925\nOct 1994       11.14643 5.121693 17.17117  1.93238821 20.36048\nNov 1994       11.14643 4.656482 17.63638  1.22090910 21.07196\nDec 1994       11.14643 4.222457 18.07041  0.55712575 21.73574\nJan 1995       11.14643 3.814079 18.47879 -0.06743481 22.36030\nFeb 1995       11.14643 3.427276 18.86559 -0.65899942 22.95187\n\n\n\n해석 : ME, RMSE … 등 알려줌. Forecasts 보면 Point Forecast는 점 추정량으로서 각각의 level에 대한 예측값 그 옆은 80% 신뢰수준으로 상한, 하한 또 그 옆은 95% 신뢰수준으로 상한, 하한을 나타낸다. 참고) Point Forecast가 전부 같은 값으로 나오는 이유는 예를 들어 100번째 데이터까지를 바탕으로 결과를 도출한다면 어짜피 100개의 결과를 바탕으로 예측한 것이기에 바뀔 이유가 없다. 하지만 신뢰구간은 시간이 지날수록 믿을 수 없는 값이 많아지기 때문에(변동 증가) 시간이 지남에 따라 구간이 커진다.\nholt는 이중지수 평활에서 사용\n\n\nfit02 <- holt(mindex, \n              alpha = 0.9,\n              beta=0, # 기울기\n              phi=0,\n              initial ='simple',\n              h = 10)\nls(fit02)\n\n\n'fitted''level''lower''mean''method''model''residuals''series''upper''x'\n\n\n\nsummary(fit02)\n\n\nForecast method: Holt's method\n\nModel Information:\nHolt's method \n\nCall:\n holt(y = mindex, h = 10, initial = \"simple\", alpha = 0.9, beta = 0,  \n\n Call:\n     phi = 0) \n\n  Smoothing parameters:\n    alpha = 0.9 \n    beta  = 0 \n    phi   = 0 \n\n  Initial states:\n    l = 9.3 \n    b = 1.4 \n\n  sigma:  2.092\nError measures:\n                     ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.02051593 2.091975 1.614844 -2.575694 16.57215 0.2947652\n                    ACF1\nTraining set -0.01462494\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80       Lo 95    Hi 95\nMay 1994       11.14643 8.465459 13.82741  7.04623710 15.24663\nJun 1994       11.14643 7.539551 14.75332  5.63018345 16.66268\nJul 1994       11.14643 6.806897 15.48597  4.50968590 17.78318\nAug 1994       11.14643 6.181200 16.11167  3.55276359 18.74010\nSep 1994       11.14643 5.625970 16.66690  2.70361248 19.58925\nOct 1994       11.14643 5.121693 17.17117  1.93238821 20.36048\nNov 1994       11.14643 4.656482 17.63638  1.22090910 21.07196\nDec 1994       11.14643 4.222457 18.07041  0.55712575 21.73574\nJan 1995       11.14643 3.814079 18.47879 -0.06743481 22.36030\nFeb 1995       11.14643 3.427276 18.86559 -0.65899942 22.95187\n\n\n\n해석: HoltWinters의 경우 원래 Point Forecast의 첫 칸 비워져서 나오는데 여기서는 0을 일부러 넣어서 칸 맞춤\n\n\nfit_data <- data.table(\n  fit_HoltWinters = c(0,fit0$fitted[,'xhat']),\n  fit_ses = fit01$fitted,\n  fit_holt = fit02$fitted\n)\n\nhead(fit_data)\n\n\n\nA data.table: 6 × 3\n\n    fit_HoltWintersfit_sesfit_holt\n    <dbl><ts><ts>\n\n\n     0.00000 9.30000 9.30000\n     9.30000 9.30000 9.30000\n    10.5600010.5600010.56000\n    13.0260013.0260013.02600\n    13.9926013.9926013.99260\n    17.4192617.4192617.41926\n\n\n\n\n\n\n\n평활지수 0.2, 0.9썼을 때 평활이 얼마나 달라지나?\n\n\ntmp.dat[, ses_0.2 := ses(mindex, alpha = 0.2)$fitted]\ntmp.dat[, ses_0.9 := ses(mindex, alpha = 0.9)$fitted]\n\nmelt.tmp <- melt(tmp.dat, id='day')\n\nggplot(melt.tmp, aes(day, value, col=variable, size=variable, lty=variable))+\n  geom_line() +\n  xlab(\"\")+ylab(\"\")+\n  theme_bw()+\n  scale_linetype_manual(values=c('solid',\"dashed\",\"dashed\" ))+\n  scale_color_manual(values=c('black','orange', 'steelblue'))+\n  scale_size_manual(values=c(0.2,1.2,1.2))+\n  guides(lty = 'none', size='none')+\n  theme(legend.position = c(0.85,0.8)) +\n  theme(legend.background = element_rect(linetype=\"solid\", \n                                         colour =\"darkblue\"))+\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n최적의 평활상수 = SSE를 최소화 시켜주는 값\n\n\nw <-c(seq(0.1,0.8,0.1), \n      seq(0.81, 0.99, 0.01)) # weight\n\nsse <- sapply(w, function(x) \n  return(sum(ses(mindex, alpha = x)$residuals^2)))\n\n\nsse %>% head\n\n\n1459.644692556371010.03396147245768.279992410308630.729514475135547.181483975014494.617442566588\n\n\n\nw[which.min(sse)]  # 최적 평활상수값\nfit1 <- ses(mindex, alpha=w[which.min(sse)], h=6)\n\n0.9\n\n\n- 예측모형 plot\n\nw1 = w[-c(1:6)]  # xaxis from 0.7 to 1.0\nsse1 = sse[-c(1:6)]\nplot(w1,sse1, type=\"o\", xlab=\"weight\", ylab=\"sse\", pch=16,\n     main=\"Sum of squares of prediction error after 1 lag\")\n\n\n\n\n\n\n\n\nt.test(resid(fit1), mu=0)\n\n\n    One Sample t-test\n\ndata:  resid(fit1)\nt = 0.088676, df = 99, p-value = 0.9295\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.3985116  0.4357973\nsample estimates:\n mean of x \n0.01864288 \n\n\n\n해당 ses의 잔차 출력한 것에 평균 0인지 t.test검증\n\n\nalpha = 0.9\n\n\nplot(fit1, xlab=\"\", ylab=\"\", \n     main=\"Intermediate Goods Shipment Index and Simple Exponential Smoothing Value alpha=0.9\", \n     lty=1,col=\"black\" )\nlines(fitted(fit1), col=\"red\", lty=2)\nlegend(\"topright\", legend=c(\"Mindex\", \"alpha=0.9\"), \n       lty=1:2,col=c(\"black\",\"red\"))\n\n\n\n\n\n마지막 보면 파란색 줄로 쭉 이어지는게 더 이상 추가해줄 데이터가 없어서 예측이 같게 이어지는 거임. 하지만 시간이 지남에 따라 신뢰구간은 늘어나기에 범위적으로 늘어나는 표현이 일어남 마지막 자세히 보면 빨간 점선이 검은 선보다 아래에 있다. alpha = 0.9라는 것은 최근 실제 정보를 9만큼, 예측값을 1만큼 내분한 점을 표현하는 것이기에 보면 파란색 줄은 검은선보다 10%정도만큼 살짝 내려온 곳에 위치하게 된다.\n\n\nplot(fit1$residuals, ylab=\"residual\",\n     main=\"Time Series Plot of Prediction Errors : alpha=0.9\"); abline(h=0)\n\n\n\n\n\nalpha = 0.2\n\n\nfit2 <- ses(mindex, alpha=0.2, h=6) \nplot(fit2, xlab=\"year\", ylab=\"mindex\", \n     main=\"Intermediate Goods Shipment Index and Simple Exponential Smoothing Value alpha=0.2\", \n     lty=1,col=\"black\")\nlines(fitted(fit2), col=\"red\", lty=2)\nlegend(\"topright\", legend=c(\"Mindex\",\"alpha=0.2\"), \n       lty=1:2,col=c(\"black\",\"red\"))\n\n\n\n\n\nplot(fit2$residuals, ylab=\"residual\",\n     main=\"Time Series Plot of Prediction Errors : alpha=0.2\"); abline(h=0)\n\n\n\n\n\n\n\n\n\nround(rbind(accuracy(fit1), accuracy(fit2)), digit=3)\n\n\n\nA matrix: 2 × 7 of type dbl\n\n    MERMSEMAEMPEMAPEMASEACF1\n\n\n    Training set 0.0192.0921.616 -2.59616.5890.295-0.015\n    Training set-0.1623.1782.595-13.10430.6500.474 0.736\n\n\n\n\n\n\n\n\nfit3 <- ses(mindex,h=6)\nfit3$model\n\nSimple exponential smoothing \n\nCall:\n ses(y = mindex, h = 6) \n\n  Smoothing parameters:\n    alpha = 0.9031 \n\n  Initial states:\n    l = 9.4594 \n\n  sigma:  2.1131\n\n     AIC     AICc      BIC \n614.1310 614.3810 621.9466 \n\n\n\nalpha 따로 입력안하면 알아서 최적으로 생각하는 alpha 입력해줌 여기서는 Smoothing parameters : alpha = 0.9031로 해줌.\n\n\nplot(fit3, xlab=\"\", ylab=\"\", main=\"\",\n     # main=\"중간재 출하지수와 단순지수평활값 : alpha estimated\", \n     lty=1, col=\"black\")\nlines(fit3$fitted, col = \"red\", lty = 2)\nlegend(\"topright\", legend=c(\"Mindex\",\"estimated alpha\"), \n       lty=1:2,col=c(\"black\",\"red\"))\nplot(fit3$residuals, ylab=\"residual\",  \n     main=\"Time Series Plot of Prediction Errors : estimated alpha\"); abline(h=0)\n\n\n\n\n\n\n\n\n\n\n\n\nz <- scan(\"stock.txt\") \nstock <- ts(z, start=c(1984,1), frequency=12)\n\ntmp.data <- data.table(\n  day = seq.Date(as.Date(\"1984-01-01\"), \n                 by='month', length.out=length(z)),\n  z=z  \n)\n\nggplot(tmp.data, aes(day, z)) + geom_line(col='skyblue') +\n  geom_point(col='steelblue')+\n  # scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y-%m\") +\n  theme_bw()+ggtitle(\"monthly stock index\")+\n  theme(plot.title = element_text(size=20),\n        axis.title = element_blank())\n\n\n\n\n\n전체적으로 증가하는 추세라고 볼 수도 있지만 증가하다가 감소하다 다시 잠잠해지는 즉, 일반적인 선형추세모형을 사용하기는 어렵다.\n\n\nplot.ts(stock, main = 'monthly stock index')\n\n\n\n\n\n\n\nalpha, beta 동일하게 사용하므로 1모수\n\n\nfit4 = holt(stock, alpha=0.6, beta=0.6, h=6) \nfit4$model\n\nHolt's method \n\nCall:\n holt(y = stock, h = 6, alpha = 0.6, beta = 0.6) \n\n  Smoothing parameters:\n    alpha = 0.6 \n    beta  = 0.6 \n\n  Initial states:\n    l = 115.6009 \n    b = 6.8098 \n\n  sigma:  40.2546\n\n     AIC     AICc      BIC \n1149.575 1149.836 1157.268 \n\n\n\nplot(fit4, ylab=\"\", xlab=\"\",  lty=1, col=\"black\",\n     main=\"Stock index and double exponential smoothing: alpha=beta=0.6\"\n     )\nlines(fitted(fit4), col=\"red\", lty=2)\nlegend(\"topleft\", lty=1:2, col=c(\"black\",\"red\"), c(\"Index\", \"Double\"), bty = \"n\")\n\n\n\n\n\n뒤에 예측한 것을 보면 마지막 시점에서 추세가 아래로 내려가고 있으므로 쭉 내려가는 것으로 예측이 된거임. 딱히 예측에 좋은 거 같지는 않음.\n\n\nplot(resid(fit4), main=\"Time series plot of forecasting error\"); abline(h=0)\n\n\n\n\n\n\n\n\nalpha, beta 선택하지 않으면 알아서 선택해서 돌려줌.\n\n\nfit5 = holt(stock, h=6)\nfit5$model\n\nHolt's method \n\nCall:\n holt(y = stock, h = 6) \n\n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.1071 \n\n  Initial states:\n    l = 124.1137 \n    b = 3.4954 \n\n  sigma:  31.8609\n\n     AIC     AICc      BIC \n1108.677 1109.343 1121.498 \n\n\n\n여기서는 0.999, 0.1로 선택됨.\nbeta를 0.6으로 쓸 때보다는 기울기가 훨씬 완만한 모습을 보인다.\n\n\nplot(fit5, ylab=\"Index\", xlab=\"year\",  lty=1, col=\"black\",\n     main=\"Intermediate Goods Shipment Index and Double Exponential Smoothing Value : alpha, beta estimated\")\nlines(fitted(fit5), col=\"red\", lty=2)\nlegend(\"topleft\", lty=1:2, col=c(\"black\",\"red\"), c(\"Index\", \"Double\"))\n\n\n\n\n\nplot(resid(fit5), main=\"Time series plot of forecasting error: alpha, beta estimated\")\nabline(h=0)\n\n\n\n\n\n\n\n\n- 항공사 데이터 사용\n\nz <- scan(\"koreapass.txt\")\npass <- ts(z, start=c(1981,1), frequency=12) \n\nplot.ts(pass)\n\ntmp.data <- data.table(\n  day = seq.Date(as.Date(\"1981-01-01\"), \n                 by='month', length.out=length(z)),\n  z=z  \n)\n\n\n\n\n\n보면 분산이 점점 증가하는 형태이기에 평활모형에서 승법모형을 사용해야 한다. or 로그변환한 가법모형사용\n\n\n\n\n가법이면 seasonal=\"additive\" 승법이면 seasonal=\"multiplicative\"\n\n\nfit6 = hw(pass, seasonal=\"additive\", h=12) # 1년 예측이라 12\nfit6$model\n\nHolt-Winters' additive method \n\nCall:\n hw(y = pass, h = 12, seasonal = \"additive\") \n\n  Smoothing parameters:\n    alpha = 0.5963 \n    beta  = 0.0177 \n    gamma = 1e-04 \n\n  Initial states:\n    l = 129763.5823 \n    b = 799.4119 \n    s = -29016.96 -855.2679 13617.18 -4624.62 37697.04 16768.22\n           9720.317 14214.29 317.4589 -5632.175 -31459.7 -20745.79\n\n  sigma:  11509.58\n\n     AIC     AICc      BIC \n2542.155 2548.955 2587.751 \n\n\n\nalpha, beta, gamma 따로 입력하지 않았으므로 알아서 해줌.\n\n\nplot(fit6,  ylab=\"passenger\", xlab=\"year\", lty=1, col=\"blue\",\n     main=\"Winters Additive Seasonal Exponential plot of Smooth Material\")\nlines(fit6$fitted, col=\"red\", lty=2)\nlegend(\"topleft\", lty=1:2, col=c(\"blue\",\"red\"), c(\"Pass\", \"Additive\"),  bty = \"n\")\n\n\n\n\n\nts.plot(resid(fit6), ylab=\"residual\", \n        main=\"Prediction error of additive model\"); abline(h=0)\n\n\n\n\n\ndwtest(lm(resid(fit6)~1), alternative = 'two.sided')\n\n\n    Durbin-Watson test\n\ndata:  lm(resid(fit6) ~ 1)\nDW = 2.0779, p-value = 0.683\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n더빈왓슨에서 ~1의 형태로 하면 상수평활형태로 결과 나옴. 여튼저튼 잔차 그 자체의 결과로 나오게 됨.\n\n\n\n\n\nfit7= hw(pass, seasonal=\"multiplicative\",h=12) \nfit7$model\n\nHolt-Winters' multiplicative method \n\nCall:\n hw(y = pass, h = 12, seasonal = \"multiplicative\") \n\n  Smoothing parameters:\n    alpha = 0.4231 \n    beta  = 0.1209 \n    gamma = 0.0014 \n\n  Initial states:\n    l = 128311.2252 \n    b = 914.1471 \n    s = 0.8553 0.9918 1.0572 0.9742 1.1913 1.0935\n           1.0469 1.0668 0.9952 0.9699 0.8389 0.9189\n\n  sigma:  0.051\n\n     AIC     AICc      BIC \n2506.102 2512.902 2551.699 \n\n\n\nplot(fit7,  ylab=\"passenger\", xlab=\"year\", lty=1, col=\"blue\",\n     main=\"Winters Time series plot of multiplicative seasonal exponential smoothed data\")\nlines(fit7$fitted, col=\"red\", lty=2)\nlegend(\"topleft\", lty=1:2, col=c(\"blue\",\"red\"), c(\"Pass\", \"Multiplicative\"),  bty = \"n\")\n\n\n\n\n\n해석 : 가법모형에서는 예측 구간에서 앞에 있는 것을 붙여 놓은 것 마냥, 사실 분산이 점점 증가하고 있기에 예측 구간의 폭도 같이 늘어나야 하는데 그렇지 못한 반면, 승법모형의 경우 분산이 커지는 것을 고려해서 진폭이 커지는 형태를 표현해준다.\n\n\nts.plot(z-fit7$fitted, ylab=\"residual\", \n        main=\"Prediction error of multiplicative model\"); abline(h=0)\n\n\n\n\n\ndwtest(lm(resid(fit7)~1), alternative = 'two.sided')\n\n\n    Durbin-Watson test\n\ndata:  lm(resid(fit7) ~ 1)\nDW = 1.6079, p-value = 0.03973\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n\n\nsmoothing을 하려고 할 때 살펴 보아야 할 것 level만 있다면 -> ses trend가 있다면 -> holt seasonality가 있다면 -> hw 그 중에서도 가법쓸 지, 승법 쓸 지"
  },
  {
    "objectID": "posts/time-series/2022-10-08-시계열자료분석-학습3.html",
    "href": "posts/time-series/2022-10-08-시계열자료분석-학습3.html",
    "title": "(수업) 시계열 자료분석 실습 3",
    "section": "",
    "text": "분해법 - 가법모형, 승법모형, 이동평균(단순, 중심, stl, decompose)\n\nlibrary('tidyverse')\nlibrary('TTR')\nlibrary('forecast')\nlibrary('lmtest')\n\n\n\n\nz <- scan(\"food.txt\")\nt <- 1:length(z)\nfood <- ts(z, start = c(1981, 1), frequency = 12)\n\n\nplot.ts(food)\n\n\n\n\n\n\n\n승법모형 써도 되지만 여기서는 로그변환하고 가법모형 사용할 예정\n\n\nlog_food <- log(food)\nplot.ts(log_food)\n\n\n\n\n\n\n\n\nfit <- lm(log_food ~ t)\nsummary(fit)\n\n\nCall:\nlm(formula = log_food ~ t)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.251154 -0.042190  0.009368  0.051058  0.147910 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 3.705715   0.012870  287.94   <2e-16 ***\nt           0.007216   0.000154   46.86   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.07682 on 142 degrees of freedom\nMultiple R-squared:  0.9393,    Adjusted R-squared:  0.9388 \nF-statistic:  2195 on 1 and 142 DF,  p-value: < 2.2e-16\n\n\n\n해석 : p-value보면 유의하고 R-squared도 매우 높게 나옴.\n\n\ntrend <- fitted(fit)\n\n\nts.plot(log_food, trend, col = 1:2, lty = 1:2, lwd = 1:2, ylab = \"food\", xlab = \n        \"Log-transformed time series and trend component by decomposition method\")\nlegend(\"topleft\", lty = 1:2, col = 1:2, c(\"ln(z)\", \"trend component\"))\n\n\n\n\n\n\n\n\n추세성분을 구했으니 원래데이터에서 추세성분을 빼준다. 그러면 계절성분과 불규칙성분만 남음. 거기서 계절성분까지 빼주려면 계절성분도 구해야한다.\n\n\nadjtrend = log_food-trend\nplot.ts(adjtrend)\n\n\n\n\n\n\n\n\n계절성분에 추세법을 이용하기 위해서는 sin함수 이용하거나 지시함수를 이용하면되는데 여기서는 지시함수 이용함. 사이클 자체를 설명변수로 넣을거임. 근데 여기서 cycle은 숫자가 아니라 1월, 2월의 의미이기에 factor형으로 바꿔준다. factor형으로 바꿔주면 Level이 생김.\n\n\ny = factor(cycle(adjtrend))\ny\n\n\n123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112\n\n\n    \n        Levels:\n    \n    \n    '1''2''3''4''5''6''7''8''9''10''11''12'\n\n\n\n\n지시함수를 사용하기 위해 여기서는 intercept를 0으로 놓았음.\n\n\nfit1 <- lm(adjtrend ~ 0 + y)\nsummary(fit1)\n\n\nCall:\nlm(formula = adjtrend ~ 0 + y)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.182321 -0.028501  0.000597  0.025663  0.146887 \n\nCoefficients:\n    Estimate Std. Error t value Pr(>|t|)    \ny1  -0.06883    0.01423  -4.837 3.61e-06 ***\ny2  -0.13853    0.01423  -9.735  < 2e-16 ***\ny3  -0.01290    0.01423  -0.907 0.366289    \ny4   0.03840    0.01423   2.699 0.007872 ** \ny5   0.08825    0.01423   6.201 6.69e-09 ***\ny6   0.03871    0.01423   2.720 0.007401 ** \ny7   0.01061    0.01423   0.746 0.457221    \ny8   0.05972    0.01423   4.197 4.94e-05 ***\ny9   0.03776    0.01423   2.653 0.008945 ** \ny10 -0.01856    0.01423  -1.304 0.194518    \ny11 -0.05041    0.01423  -3.542 0.000549 ***\ny12  0.01577    0.01423   1.108 0.269816    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.0493 on 132 degrees of freedom\nMultiple R-squared:  0.6172,    Adjusted R-squared:  0.5824 \nF-statistic: 17.73 on 12 and 132 DF,  p-value: < 2.2e-16\n\n\n\nseasonal <- fitted(fit1)\nts.plot(seasonal, main = \"Estimated seasonal component\")\n\n\n\n\n\n\n\n\npred <- trend + seasonal\n\n\ntrend는 단순선형을 사용해서 구했고, seasoanl은 지시함수를 사용해서 구했음.\n\n\nts.plot(log_food, pred, col = 1:2, lty = 1:2, lwd = 1:2, ylab = \"food\", xlab = \"time\",\n        main=\"Estimated value by log-transformed time series and decomposition method\")\nlegend(\"topleft\", lty = 1:2, col = 1:2, c(\"ln(z)\", \"estimated value\"))\n\n\n\n\n\n\n\n\n최종적으로 잔차에 대해서 보려고 했기에 살펴보면\n\n\nirregular <- log_food - pred\nts.plot(irregular)\nabline(h = 0)\n\n\n\n\n\n겉 보기에 문제는 별로 없어보이지만 더빈왓슨 테스트를 해보면\n\n\ndwtest(lm(irregular ~ 1), alternative = 'two.sided')\n\n\n    Durbin-Watson test\n\ndata:  lm(irregular ~ 1)\nDW = 1.0803, p-value = 2.748e-08\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n자기상관관계가 있다고 나옴, 이런건 모델링을 해주어야하는데 5장 이후에 알려줌.\n\n\n\n\n\n\nplot.ts(food)\n\n\n\n\n\n\n\ntrend 잘 안잡힌것 같아서 t^2추가해도 됨. 이거 lm에 추가할 때 I()하고 넣어야 들어감. 바로 쓰면 추가 안됨.\n\n\nfit3 <- lm(food ~ t) #+ I(t^2))\nsummary(fit3)\n\n\nCall:\nlm(formula = food ~ t)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.0331  -3.4505  -0.1355   4.2911  15.3948 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 35.28614    0.95561   36.92   <2e-16 ***\nt            0.50557    0.01143   44.21   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.704 on 142 degrees of freedom\nMultiple R-squared:  0.9323,    Adjusted R-squared:  0.9318 \nF-statistic:  1955 on 1 and 142 DF,  p-value: < 2.2e-16\n\n\n\ntrend <- fitted(fit3)\n\n\nts.plot(food, trend, col = 1:2, lty = 1:2, ylab = \"food\", xlab = \"time\",\n        main = \"Trend component by primitive series and decomposition method\")\nlegend(\"topleft\", lty = 1:2, col = 1:2, c(\"primitive series\", \"trend component\"))\n\n\n\n\n\n가법모형과 비슷해 보이지만 다름.\n\n\n\n\n\n나눠주면 계절성분과 불규칙 성분만 남음.\n\n\nadjtrend = food/trend\nplot.ts(adjtrend)\n\n\n\n\n\n\n\n\ny = factor(cycle(adjtrend))\n\n\n추세가 제거된 모형\n\n\nfit4 <- lm(adjtrend ~ 0+y)\nfit4 %>% summary\n\n\nCall:\nlm(formula = adjtrend ~ 0 + y)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.161461 -0.039085 -0.006972  0.032830  0.296240 \n\nCoefficients:\n    Estimate Std. Error t value Pr(>|t|)    \ny1   0.94148    0.01922   48.98   <2e-16 ***\ny2   0.87529    0.01922   45.54   <2e-16 ***\ny3   0.99012    0.01922   51.51   <2e-16 ***\ny4   1.04123    0.01922   54.17   <2e-16 ***\ny5   1.09340    0.01922   56.88   <2e-16 ***\ny6   1.04083    0.01922   54.15   <2e-16 ***\ny7   1.01069    0.01922   52.58   <2e-16 ***\ny8   1.06149    0.01922   55.22   <2e-16 ***\ny9   1.03869    0.01922   54.04   <2e-16 ***\ny10  0.98045    0.01922   51.01   <2e-16 ***\ny11  0.94939    0.01922   49.39   <2e-16 ***\ny12  1.01452    0.01922   52.78   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.06658 on 132 degrees of freedom\nMultiple R-squared:  0.996, Adjusted R-squared:  0.9956 \nF-statistic:  2733 on 12 and 132 DF,  p-value: < 2.2e-16\n\n\n\nseasonal <- fitted(fit4)\nts.plot(seasonal, main = \"Estimated seasonal component\")\n\n\n\n\n\n\n\n\npred <- trend * seasonal\n\n\nts.plot(food, pred, col = 1:2, lty = 1:2, ylab = \"food\", xlab = \"time\",\n        main = \"Estimated value by primitive series and decomposition method\")\nlegend(\"topleft\", lty = 1:2, col = 1:2, c(\"primitive series\", \"estimated value\"))\n\n\n\n\n\n\n\n\nirregular <- food/pred\n\n\nts.plot(irregular)\nabline(h = 1, lty = 2)\n\n\n\n\n\ndwtest(lm(irregular ~ 1), alternative = 'two.sided')\n\n\n    Durbin-Watson test\n\ndata:  lm(irregular ~ 1)\nDW = 0.57136, p-value < 2.2e-16\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n\n\n\n- 원래 데이터\n\nz <- scan('mindex.txt')\nmindex <- ts(z, start = c(1986, 1), frequency = 12)\nplot.ts(mindex)\n\n\n\n\n\n단순이동평균 : smoothing해주려고 사용 예를 들어 단순이동평균의 경우 window가 5라고 하면 앞에 4개는 결과가 없음. but, 중심이동평균은 5개이면 위는 4번째까지 합해서 5번째부터 결과 나타나는 식이라면 중심이동은 5개의 중심값 3번째부터 결과나옴. 좀 더 유연한 결과 예측에서는 사용 불가, 왜냐하면 5개의 경우 하나는 현재 2개는 과거 2개는 미래정보를 사용하기에\n\n\nplot.ts(mindex, ylab = \"\", xlab = \"\")\nlines(SMA(mindex, n = 5), col = 'red', lwd = 2) #처음 4개 안나오는 거 볼 수 있음\nlines(ma(mindex, order = 5), col = 'blue', lty = 2, lwd = 2) # 두칸 일찍 나오는 거 보임\nlegend('topright', lty = c(1,1,2), col = c('black', 'red', 'blue'),\n       lwd = c(1, 1, 2),\n       c('primitive series', \"SMA(m=5)\", \"CSMA(l=2)\"),\n       bty = 'n')\n\n\n\n\n\n\n\n\nplot.ts(log_food)\nlines(ma(log_food, 3), col = 'blue', lwd = 2)\nlines(ma(log_food, 12), col = 'red', lwd = 2) # 거의 추세만 남아버림.(왜냐면 12개씩이면 1년씩이 묶인거라 계절성이 없어짐)\n\n\n\n\n\n함수 stl, decompose 두개 다 사용가능\n\n\n\n\n원하는 3개의 성분으로 쪼개서 보여줌\n\n\nstl_fit1 <- stl(log_food, s.window = 12)\nstl_fit1$time.series %>% head\n\n\n\nA Time Series: 6 × 3\n\n    seasonaltrendremainder\n\n\n    Jan 1981-0.090355043.789108 0.09223150\n    Feb 1981-0.147931393.787232 0.04957932\n    Mar 1981-0.019896143.785355-0.03017285\n    Apr 1981 0.038295803.783478-0.04901299\n    May 1981 0.094538723.782254-0.04815173\n    Jun 1981 0.043787293.781031-0.01593573\n\n\n\n\n\nplot(stl_fit1)\n\n\n\n\n\n실제 예측값은 trend + seasonal\n\n\npred_stl <- stl_fit1$time.series[,1] + stl_fit1$time.series[,2]\n\nts.plot(log_food, pred_stl, col = 1:2, lty = 1:2, ylab = \"food\", xlab = \"time\",\n        main = \"Estimated value by primitive series and decomposition method\")\nlegend(\"topleft\", lty = 1:2, col = 1:2, c(\"primitive series\", \"estimated value\"))\n\n\n\n\n\n\n\n\n밑에 additive써줬는데 안써도 되긴함.\n\n\ndec_fit <- decompose(log_food, 'additive')\ndec_fit$trend[1:10]\nma(log_food, 12)[1:10]\n\n\n<NA><NA><NA><NA><NA><NA>3.776909155269433.768171473872113.765649627472043.77219261288612\n\n\n\n<NA><NA><NA><NA><NA><NA>3.776909155269433.768171473872113.765649627472043.77219261288612\n\n\n\ndec_fit$seasonal[1:24]\n\n\n-0.0799020938844753-0.146043800620445-0.01274493183601310.04049677614460120.09068184389565770.03967866993284980.01048177141592280.0587619091303550.041341551797343-0.0179374241117825-0.04690019248569370.0220859206216802-0.0799020938844753-0.146043800620445-0.01274493183601310.04049677614460120.09068184389565770.03967866993284980.01048177141592280.0587619091303550.041341551797343-0.0179374241117825-0.04690019248569370.0220859206216802\n\n\n\n계절성분이라는 것은 12개월마다 반복되는 것이라 12까지 돌고 똑같이 나옴.\n\n\nplot.ts(log_food - dec_fit$trend)\n\n\n\n\n- 1~12월까지의 평균\n\nx <- log_food - dec_fit$trend\nb <- tapply(x, cycle(x), function(y) mean(y, na.rm = T))\nb - mean(b)\n\n1-0.07990209388447532-0.1460438006204453-0.012744931836013140.040496776144601250.090681843895657760.039678669932849870.010481771415922880.05876190913035590.04134155179734310-0.017937424111782511-0.0469001924856937120.0220859206216802\n\n\n\ndec_fit$random[1:10]\n\n\n<NA><NA><NA><NA><NA><NA>-0.00547660660422888-0.0314441938302674-0.03883854426093690.0276591313067893\n\n\n\nplot(dec_fit)\n\n\n\n\n\npredict = trend + seasonal\n\n\npred_dec <- dec_fit$trend + dec_fit$seasonal\n\n\nts.plot(log_food, pred_dec, col = 1:2, lty = 1:2, ylab = \"food\", xlab = \"time\",\n        main = \"Estimated value by primitive series and decomposition method\")\nlegend(\"topleft\", lty = 1:2, col = 1:2, c(\"primitive series\", \"Estimated value\"))\n\n\n\n\n\n\n\n\nts.plot(pred_stl, pred_dec, col = 1:2, lty = 1:2, ylab = \"food\", xlab = \"time\",\n        main = \"stl vs decompose\")\nlegend(\"topleft\", lty = 1:2, col = 1:2, c(\"stl\", \"decompose\"))\n\n\n\n\n\n거의 비슷하게 나온다?\n\n\n\n\n\ndec_fit2 <- decompose(food, type = \"multiplicative\")\ndec_fit2$trend[1:10]\ndec_fit2$seasonal[1:15]\ndec_fit2$random[1:10]\n\n\n<NA><NA><NA><NA><NA><NA>43.729166666666743.379166666666743.291666666666743.5875\n\n\n\n0.9219246356829520.8621213373421160.9844960475951441.038004198013531.091650489534581.037859336820561.008002905607491.059170985651181.042754035631620.9807591720335130.9523824968151361.020874359272170.9219246356829520.8621213373421160.984496047595144\n\n\n\n<NA><NA><NA><NA><NA><NA>0.995936238493310.9685292269139350.9591835261281731.0269284401149\n\n\n\nplot(dec_fit2)\n\n\n\n\n\npred_dec2 <- dec_fit2$trend*dec_fit2$seasonal\n\n\nts.plot(food, pred_dec2, col = 1:2, lty = 1:2, ylab = \"food\", xlab = \"time\",\n        main = \"Estimated value by primitive series and decomposition method\")\nlegend(\"topleft\", lty = 1:2, col = 1:2, c(\"primitive series\", \"estimated value\"))\n\n\n\n\n\n\n\n\nts.plot(exp(pred_dec), pred_dec2, col = 1:2, lty = 1:2, ylab = \"food\", xlab = \"time\",\n        main = \"Estimated value by primitive series and decomposition method\")\nlegend(\"topleft\", lty = 1:2, col = 1:2, c(\"primitive series\", \"estimated value\"))\n\n\n\n\n\n비슷하게 나온다. 추세모형은 회귀모형을 써서 예측값이 좀 다르게 나오는데 이건 좀 비슷하다.\n결론 : 이분산성이 있는 경우 승법모형을 써야되지만 로그변환이후 가법모형을 사용해도 된다. 분해법에는 추세이용, 평활법이 있다. 평활법을 이용할 때에는 일반적인 이동법과 중심이동법이 있는데 분해법에서 사용하는 평활을 위해서는 중심이동평균 쓰는 것이 더 좋다."
  },
  {
    "objectID": "posts/time-series/2022-10-14-시계열자료분석-학습5.html",
    "href": "posts/time-series/2022-10-14-시계열자료분석-학습5.html",
    "title": "(수업) 시계열 자료분석 실습 5",
    "section": "",
    "text": "AR, MA, ARMA\n\nlibrary('tidyverse')\nlibrary('gridExtra')\nlibrary('forecast')\n\n\n\n\n\n\n함수로 생성하기\nn개 보다 100개 더 생성하는 이유 : 초기값을 만들고 업데이트할 때 초기값의 영향을 줄여주기 위해 앞에 100개 없애기 위해서 쌩으로 n개만 쓰면 초기값 영향 많이 받음.\n\n\nsim_ar1 <- function(n, phi, mu, sigma){\n    z <- rep(mu, n + 100)\n    for(k in 2 : (n + 100)){\n        z[k] <- mu + phi*(z[k-1] - mu) + rnorm(1, 0, sigma)\n        }\n    return(z[-(1:100)])\n    }\n\n- 전체중에서 앞에꺼 100개 떼고 생성\n\n정상시계열이 되기 위해서는 \\(\\phi\\)가 1보다 작기만 하면 됨. 평균은 그냥 간단하게 0으로 함.\n\n\nn <- 100\nphi <- -0.5\nmu <- 0\nsigma <- 1\n\n\ntmp.data <- data.frame(\n    t = 1:n,\n    z = sim_ar1(n, phi, mu, sigma)\n    )\n\n\np1 <- ggplot(tmp.data, aes(t, z)) +\ngeom_line(col = 'steelblue', lwd = 1.2) +\ngeom_hline(yintercept = 0, lty = 2, col = 'grey') +\nggtitle(paste0(\"Time series plot : AR(1) - phi = \", phi)) +\ntheme_bw() +\ntheme(axis.title.y = element_blank())\n\np2 <- ggAcf(tmp.data$z, lwd = 1.5) +\ntheme_bw() + ylim(-1, 1) + \nggtitle(\"SACF\") +\ntheme(axis.title.y = element_blank())\n\np3 <- ggPacf(tmp.data$z, lwd = 1.5) +\ntheme_bw() + ylim(-1, 1) +\nggtitle('SPACF') +\ntheme(axis.title.y = element_blank())\n\n\np1\np2\np3\n\n\n\n\n\n\n\n\n\n\n\n해석 : 0을 기준으로 대칭적으로 움직임(정상 시계열: 추세도 없고 계절도 없어보임), AR(1)은 pacf를 봐야하는데 첫 번째와 두 번째가 유의하게 나옴. 근데 원래 알기로는 첫 번째만 유의하고 두 번째부터 유의하지 않아야하는데 이거는 100개의 sampling이라 약간의 오차 표본을 10000개씩 뽑아보면 이론에 근접하게 나옴.\n\n\n\n\n\\(Z_t = \\delta + \\phi_1*Z_{t-1} + ... + \\phi_p * Z_{t-p} + \\epsilon_t\\)\n\nsim_ar <- function(n, mu, phi, sigma){\n    p <- length(phi) # 차수\n    z <- rep(mu, (100 + n))\n    delta <- (1-sum(phi))*mu \n    \n    for(k in (length(phi) + 1) : (n + 100)){\n        \n        z[k] <- delta + sum(z[(k-1):(k-p)]*phi) + rnorm(1, 0, sigma)\n        }\n    return(z[-(1:100)])\n    }\n\n- 예를들어 AR(1)면\n\nz <- sim_ar(100, 0, phi = c(0.5), 1)\n\n\nlayout.matrix <- matrix(c(1, 2, 1, 3), nrow = 2, ncol = 2)\nlayout(mat = layout.matrix)\nplot.ts(z)\nacf(z)\npacf(z)\n\n\n\n\n\nplot.ts(z)\nacf(z)\npacf(z)\n\n\n\n\n\n\n\n\n\n\n- 예를들어 AR(2)면\n\nz <- sim_ar(100, 0, phi = c(1.3, -0.7), 1)\n\nplot.ts(z)\nacf(z)\npacf(z)\n\n\n\n\n\n\n\n\n\n\n\n해석 : pacf보면 2개만 유의한 것을 볼 수 있음.\n\n\n\n\n\n저렇게 손으로 코드 안짜도 되는 것이 이미 패키지 모델 만들어져 있음.\n\n\nz <- arima.sim(n = 100, # 이름이 .sim인 이유는 simulation이라서\n               list(order = c(1, 0, 0), ar = 0.5), # order = c(p, d, q)인데 ARMA는 d가 0인거임, 여기는 AR이니 d, q를 0으로\n               # 뒤에 ar = phi의미\n               rand.gen = rnorm)\n\nplot.ts(z)\nacf(z, lag.max = 24)\npacf(z, lag.max = 24)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nz <- arima.sim(n = 100,\n               list(order = c(2, 0, 0), ar = c(0.5, -0.4)), # sd옵션 추가하면 분산도 변경가능 궁금한 것은 help가서 보기\n               rand.gen = rnorm)\n\nplot.ts(z)\nacf(z, lag.max = 24)\npacf(z, lag.max = 24)\n\n\n\n\n\n\n\n\n\n\n\nggplot으로 그리기\n\n\np1 <- ggplot(data.frame(t = 1:length(z), z = as.numeric(z)), aes(t, z)) +\ngeom_line(col = 'steelblue', lwd = 1.2) +\ngeom_hline(yintercept = 0, lty = 2, col = 'grey') +\nggtitle(\"Time series plot : AR(1)\") +\ntheme_bw() +\ntheme(axis.title.y = element_blank())\n\np2 <- ggAcf(z, lwd = 1.5) +\ntheme_bw() + ylim(-1, 1) +\nggtitle(\"SACF\") +\ntheme(axis.title.y = element_blank())\n\np3 <- ggPacf(z, lwd = 1.5) +\ntheme_bw() + ylim(-1, 1) +\nggtitle(\"SPACF\") +\ntheme(axis.title.y = element_blank())\n\ngrid.arrange(p1, p2, p3, nrow = 2,\n             layout_matrix = rbind(c(1, 1),\n                                   c(2, 3)))\n\n\n\n\n\n\n\n\n\n\n\n함수로 생성하기\n\n\\(Z_t = \\epsilon_t - \\theta_1 * \\epsilon_{t-1} + ... + \\theta_q * \\epsilon_{t-q}\\)\n\nsim_ma <- function(n, mu, theta, sigma){\n    q <- length(theta)\n    ep <- rnorm(n + 100, 0, sigma)\n    z <- ep\n    \n    for(k in (q+1) : (n + 100)){\n        z[k] <- mu + ep[k] - sum(ep[(k-1):(k-q)]*theta)\n        }\n    return(z[-(1:100)])\n    }\n\n- 예를들어 MA(1)면\n\nz <- sim_ma(100, 0, theta = 0.9, 1)\n\nts.plot(z)\nacf(z)\npacf(z)\n\n\n\n\n\n\n\n\n\n\n\n해석 : MA의 경우 acf보면 되는데 2번째 것까지 유의하게 나오니 맞다.\n이 역시 패키지로 만들어져 있다.\n\n\n\n\n\nz <- arima.sim(n = 100,\n               list(order = c(0,0,1), ma = -0.9), # 이거 help가서 패키지 짜여진 식을 보면 위에서 손으로 짠 식과 부호가 반대임 그래서 \"-\"붙임.\n               rand.gen = rnorm)\n\nts.plot(z)\nacf(z, lag.max = 24)\npacf(z, lag.max = 24)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nz <- arima.sim(n = 100,\n               list(order = c(0,0,2), ma = c(0.5, -0.2)),\n               rand.gen = rnorm)\n\nts.plot(z)\nacf(z, lag.max = 20)\npacf(z, lag.max = 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n역시 함수 짜서 해도 된다. 여기서는 패키지 써서\n\n\nz <- arima.sim(n = 10000, list(order = c(1, 0, 1),\n                               ar = -0.5, ma = -0.3),\n               rand.gen = rnorm)\n\nplot.ts(z)\nacf(z, lag.max = 20)\npacf(z, lag.max = 20)\n\n\n\n\n\n\n\n\n\n\n\n해석 : acf, pacf 둘 다 감소하고 있다. -> “ARMA모형이네” 참고로 위의 그림에서 acf로만 보면 MA(5)정도를 사용해야할 것 같고, pacf로만 보면 AR(4)정도 사용해야 할 것 같은데 종합해서 ARMA(1, 1)를 사용해버리면 추정해야될 것이 5개, 4개에서 2개로 줄어들기에 더 이득이라 볼 수 있다."
  }
]